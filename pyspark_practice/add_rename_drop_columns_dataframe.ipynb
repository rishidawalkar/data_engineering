{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d020a5ab-f023-47f5-91ab-5bdddc2a0256",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark, os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47cb24dd-917b-47bb-8e99-964cbe80ca13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26fcf4e3-5996-4c2c-bd53-80ed25b02e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StringType,IntegerType, StructField\n",
    "from pyspark import SparkConf,SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49a1be0f-bd01-4515-98c9-f27f7c86a840",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ec37298-e861-44a6-b83a-77de5c3f93b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------------+------------+-------------+-------+--------+-----------+-------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "| Id|Institution_Name| Branch_Name|Branch_Number|   City|  County|      State|Zipcode|2010_Deposits|2011_Deposits|2012_Deposits|2013_Deposits|2014_Deposits|2015_Deposits|2016_Deposits|\n",
      "+---+----------------+------------+-------------+-------+--------+-----------+-------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "|  1|           abcde|      Bandra|          100| Mumbai| Western|Maharashtra| 400000|      9783849|       844848|      8484393|       834841|       284773|      7389298|       737839|\n",
      "|  2|            gdtd|Govind nagar|          101| Nashik| Eastern|Maharashtra|  41111|        64848|       836239|        53446|       834780|        76436|        98753|        23556|\n",
      "|  3|            huhu|Shivaji park|          102| Mumbai| Western|Maharashtra| 400000|       738282|       618151|       878776|       676755|       343567|      2457445|       764538|\n",
      "|  4|            gstu|       Bronx|          500|Newyork|Northern|         NY|  10000|      7632927|       652818|       787565|      1234556|      4545734|     67545454|         4543|\n",
      "|  5|           ppaio|   Manhattan|          501|Newyork| Central|         NY|  10001|       729929|       638828|       517829|      4291873|       743778|       122738|       737728|\n",
      "+---+----------------+------------+-------------+-------+--------+-----------+-------+-------------+-------------+-------------+-------------+-------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=spark.read.format(\"csv\").option(\"header\",True).load(\"sample1.csv\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "917ca297-9138-4b91-a100-fbfd3cc57d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Id: int, Name: string, Salary: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emp_data=[(1,'Rishi',29409),(2,\"Nachi\",294785),(3,\"Mahesh\",45663),(4,\"Akash\",73639)]\n",
    "\n",
    "#emp_schema=[\"Id\",\"Name\",\"Salary\"]\n",
    "\n",
    "emp_schema=StructType([StructField('Id', IntegerType(), True),\n",
    "                          StructField('Name',StringType(), True),\n",
    "                          StructField('Salary',IntegerType(), True)])\n",
    "\n",
    "df1=spark.createDataFrame(data=emp_data, schema=emp_schema)\n",
    "\n",
    "display(df1)\n",
    "\n",
    "#df1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "170b17dc-3fd6-4674-9d27-7b0857636ccb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'options'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#adding new column to dataframe\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lit\n\u001b[1;32m----> 3\u001b[0m df_add\u001b[38;5;241m=\u001b[39m\u001b[43mdf1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLocation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmumbai\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minferSchema\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\pythonlab\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:3123\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   3090\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001b[39;00m\n\u001b[0;32m   3091\u001b[0m \n\u001b[0;32m   3092\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3120\u001b[0m \u001b[38;5;124;03m+---+\u001b[39;00m\n\u001b[0;32m   3121\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m-> 3123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   3124\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name)\n\u001b[0;32m   3125\u001b[0m     )\n\u001b[0;32m   3126\u001b[0m jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mapply(name)\n\u001b[0;32m   3127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(jc)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'options'"
     ]
    }
   ],
   "source": [
    "#adding new column to dataframe\n",
    "from pyspark.sql.functions import lit\n",
    "df_add=df1.withColumn(\"Location\", lit(\"mumbai\")).options(\"inferSchema\", True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e2527fa5-e751-4829-8ada-da5039930fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Id: bigint, First Name: string, Salary: bigint]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#column renaming\n",
    "df1=df1.withColumnRenamed(\"Name\",\"First Name\")\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2ebf50df-2b34-42a5-b94f-fc37b0ddc74b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Name` cannot be resolved. Did you mean one of the following? [`Id`, `Salary`, `First Name`].;\n'Project [Id#267L, First Name#390, Salary#269L, concat(Id#267L,  , 'Name) AS Id and Name#395]\n+- Project [Id#267L, Name#268 AS First Name#390, Salary#269L]\n   +- LogicalRDD [Id#267L, Name#268, Salary#269L], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#concating multiple columns and create a new column with those combined values seperated by space using lit(\" \")\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mdf1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mId and Name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mId\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mlit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mName\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\pythonlab\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:5170\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[1;34m(self, colName, col)\u001b[0m\n\u001b[0;32m   5165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[0;32m   5166\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m   5167\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5168\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m   5169\u001b[0m     )\n\u001b[1;32m-> 5170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[1;32m~\\pythonlab\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\pythonlab\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `Name` cannot be resolved. Did you mean one of the following? [`Id`, `Salary`, `First Name`].;\n'Project [Id#267L, First Name#390, Salary#269L, concat(Id#267L,  , 'Name) AS Id and Name#395]\n+- Project [Id#267L, Name#268 AS First Name#390, Salary#269L]\n   +- LogicalRDD [Id#267L, Name#268, Salary#269L], false\n"
     ]
    }
   ],
   "source": [
    "#concating multiple columns and create a new column with those combined values seperated by space using lit(\" \")\n",
    "df1.withColumn(\"Id and Name\", concat(\"Id\",lit(\" \"),\"Name\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6f8114bc-974c-4c96-9eb0-bbde7aaed631",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the column from the dataframe\n",
    "df1=df1.drop(\"Salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a2af5e17-cce9-4995-bef9-a710919b2f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Id: bigint, First Name: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
