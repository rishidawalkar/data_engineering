{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e134bb51-e774-45b1-a4be-fe140d4b2b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark, os, sys\n",
    "from pyspark.sql import *\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql.functions import when, expr\n",
    "from pyspark.sql.types import StructType, StringType,IntegerType, StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef1630d9-622d-4042-b383-98ff276aadc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b659760a-c4ac-4b55-9c7d-7cbcc7c2e215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Id: int, Name: string, Salary: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "emp_data=[(1,'Rishi',29409),(2,\"Nachi\",294785),(3,\"Mahesh\",45663),(4,\"Akash\",73639)]\n",
    "\n",
    "emp_schema=StructType([\n",
    "        StructField('Id',IntegerType(), True),\n",
    "        StructField('Name', StringType(), True),\n",
    "        StructField('Salary', IntegerType(), True)\n",
    "])\n",
    "\n",
    "emp_df=spark.createDataFrame(data=emp_data, schema=emp_schema)\n",
    "\n",
    "display(emp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0451c33a-d50e-4cd7-8300-601950e99b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Id: int, Org_name: string, est_year: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "org_data=[(1,'BNY',1998),(2,\"JPM\",1880),(3,\"HSBC\",2011)]\n",
    "\n",
    "org_schema=StructType([\n",
    "        StructField('Id',IntegerType(), True),\n",
    "        StructField('Org_name', StringType(), True),\n",
    "        StructField('est_year', IntegerType(), True)\n",
    "])\n",
    "\n",
    "org_df=spark.createDataFrame(data=org_data, schema=org_schema)\n",
    "\n",
    "display(org_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a66b1858-78ee-4db8-ab64-2082a5a32af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Id: int, Name: string, Salary: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Till spark 2.0, 'union' is used to combine all unique records from both dataframe, while 'unionall' is used to combine all the records including duplicates from both dataframes. But after spark 2.0, both 'union' and 'unionall' perform same operations.\n",
    "df_union=emp_df.union(org_df)\n",
    "display(df_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b1b2f8b-1d41-4711-8d36-8c216661d236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Id: int, Name: string, Salary: int]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#to drop duplicate data, we use below operation\n",
    "df_union.dropDuplicates()\n",
    "display(df_union)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
