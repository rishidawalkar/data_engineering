{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea18e291-1395-420b-b843-0739d9b74ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-------------+\n",
      "|   City|grouped_city|activity_rank|\n",
      "+-------+------------+-------------+\n",
      "| Mumbai|           2|            1|\n",
      "|Newyork|           2|            2|\n",
      "| Nashik|           1|            3|\n",
      "+-------+------------+-------------+\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `user_id` cannot be resolved. Did you mean one of the following? [`City`, `Id`, `State`, `County`, `Zipcode`].;\n'Aggregate ['user_id], ['user_id, sum('listen_duration) AS total_listening#93, 'count(distinct 'song_id) AS distinct_songs#94]\n+- Relation [Id#17,Institution_Name#18,Branch_Name#19,Branch_Number#20,City#21,County#22,State#23,Zipcode#24,2010_Deposits#25,2011_Deposits#26,2012_Deposits#27,2013_Deposits#28,2014_Deposits#29,2015_Deposits#30,2016_Deposits#31] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 41\u001b[0m\n\u001b[0;32m     30\u001b[0m df_final\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03mQuestion 2:\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124;03mConvert below query to pyspark code:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03mgroup by user_id;\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m df_group\u001b[38;5;241m=\u001b[39m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlisten_duration\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtotal_listening\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcountDistinct\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msong_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdistinct_songs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m df_final\u001b[38;5;241m=\u001b[39mdf_group\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_listening\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mround\u001b[39m(df_group\u001b[38;5;241m.\u001b[39mtotal_listening\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m))\n\u001b[0;32m     43\u001b[0m df_final\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\pythonlab\\Lib\\site-packages\\pyspark\\sql\\group.py:186\u001b[0m, in \u001b[0;36mGroupedData.agg\u001b[1;34m(self, *exprs)\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(c, Column) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m exprs), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall exprs should be Column\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    185\u001b[0m     exprs \u001b[38;5;241m=\u001b[39m cast(Tuple[Column, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m], exprs)\n\u001b[1;32m--> 186\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jgd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession)\n",
      "File \u001b[1;32m~\\pythonlab\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\pythonlab\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `user_id` cannot be resolved. Did you mean one of the following? [`City`, `Id`, `State`, `County`, `Zipcode`].;\n'Aggregate ['user_id], ['user_id, sum('listen_duration) AS total_listening#93, 'count(distinct 'song_id) AS distinct_songs#94]\n+- Relation [Id#17,Institution_Name#18,Branch_Name#19,Branch_Number#20,City#21,County#22,State#23,Zipcode#24,2010_Deposits#25,2011_Deposits#26,2012_Deposits#27,2013_Deposits#28,2014_Deposits#29,2015_Deposits#30,2016_Deposits#31] csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''Question. 1\n",
    "Covert below query to Pyspark code:\n",
    "\n",
    "select City , count(City) as grouped_city, \n",
    "row_number() over (order by count(City) desc) activity_rank \n",
    "from table \n",
    "group by City;\n",
    "'''\n",
    "\n",
    "import pyspark, os, sys\n",
    "from pyspark.sql import *\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import types\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "spark=SparkSession.builder.getOrCreate()\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"inferSchema\", True).option(\"header\", True).load(\"sample1.csv\")\n",
    "#df.show()\n",
    "\n",
    "\n",
    "df_1=df.select(\"City\").groupBy(\"City\").agg(count(\"City\").alias(\"grouped_city\"))\n",
    "#df_1.show()\n",
    "\n",
    "window_df=Window.orderBy(col(\"grouped_city\").desc())\n",
    "df_final=df_1.withColumn(\"activity_rank\", row_number().over(window_df))\n",
    "df_final.show()\n",
    "\n",
    "'''\n",
    "Question 2:\n",
    "Convert below query to pyspark code:\n",
    "\n",
    "select user_id, round(sum(listen_duration)/60) as 'total_listening', count(distinct(song_id)) as 'distinct_songs'\n",
    "from listening_habits\n",
    "group by user_id;\n",
    "'''\n",
    "\n",
    "df_group=df.groupBy(\"user_id\").agg(sum(\"listen_duration\").alias(\"total_listening\"),countDistinct(\"song_id\").alias(\"distinct_songs\"))\n",
    "df_final=df_group.withColumn(\"total_listening\", round(df_group.total_listening/60))\n",
    "df_final.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 3:\n",
    "Convert below query to pyspark code:\n",
    "\n",
    "with cte as\n",
    "(\n",
    "select *, lead(ID) over (order by ID) as next_id,\n",
    "lag(ID) over (order by ID) as prev_id\n",
    "from Students  \n",
    ") \n",
    "select case\n",
    "when ID%2!=0 and next_id is not null then next_id\n",
    "when ID%2=0 then prev_id\n",
    "when ID%2!=0 and next_id is null then ID\n",
    "end\n",
    "as ID, name\n",
    "from cte;\n",
    "'''\n",
    "\n",
    "window_df=Window.orderBy(col('ID'))\n",
    "df=spark.read.format(\"csv\").option(\"inferSchema\", True).option(\"header\", True).load(\"example_student.csv\")\n",
    "\n",
    "result_df=df.withColumn(\"next_id\", lead(\"ID\").over(window_df)).withColumn(\"prev_id\", lag(\"ID\").over(window_df))\n",
    "\n",
    "df1=result_df.withColumn(\"result_id\", when((df.ID%2!=0) & (result_df.next_id.isNotNull()), result_df.next_id)\n",
    "                                        .when(df.ID%2==0, result_df.prev_id)\n",
    "                                        .when((df.ID%2!=0) & (result_df.next_id.isNull()), df.ID))\n",
    "\n",
    "df1.select(\"Name\", \"result_id\").show()\n",
    "\n",
    "\n",
    "'''\n",
    "Question 4:\n",
    "Convert below query to pyspark code:\n",
    "\n",
    "select employees.id, employees.name from\n",
    "employees left join terminations\n",
    "on employees.id=terminations.emp_id\n",
    "where (terminations.term_date is null or terminations.term_date>'2016-01-02')\n",
    "order by employees.hire_date;\n",
    "'''\n",
    "\n",
    "emp_df=spark.read.format(\"csv\").option(\"header\", True).option(\"inferSchema\", True).load(\"emp1.csv\")\n",
    "term_df=spark.read.format(\"csv\").option(\"header\", True).option(\"inferSchema\", True).load(\"term1.csv\")\n",
    "result_df=emp_df.join(term_df, emp_df.id==term_df.emp_id, \"left\").filter((col(\"term_date\").isNull()) | (col(\"term_date\")> '2016-01-02')).orderBy(\"hire_date\")\n",
    "\n",
    "result_df.select(\"id\", \"name\").show()\n",
    "\n",
    "\n",
    "'''\n",
    "Question 5:\n",
    "Convert below query into pyspark code:\n",
    "\n",
    "select * from customers\n",
    "where id not in \n",
    "(\n",
    "select customers.id from customers inner join orders1\n",
    "on customers.id = orders1.customerId\n",
    ")\n",
    "order by customers.name;\n",
    "'''\n",
    "\n",
    "cust_df=spark.read.format(\"csv\").option(\"inferSchema\", True).option(\"header\", True).load(\"cust1.csv\")\n",
    "order_df=spark.read.format(\"csv\").option(\"inferSchema\", True).option(\"header\", True).load(\"order1.csv\")\n",
    "\n",
    "join_df=cust_df.join(order_df, cust_df.id==order_df.customerId, \"inner\")\n",
    "\n",
    "cust_with_order_df=join_df.select(cust_df[\"id\"])\n",
    "\n",
    "filtered_customer_df= cust_df.filter(~cust_df[\"id\"].isin([row.id for row in cust_with_order_df.collect()]))\n",
    "\n",
    "result_df=filtered_customer_df.orderBy(\"name\")\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "\n",
    "'''\n",
    "Question 6:\n",
    "Convert below query into pyspark code:\n",
    "\n",
    "with result as (\n",
    "select distinct caller_id, count(caller_id)\n",
    "from calls c inner join users u\n",
    "on c.caller_id=u.user_id\n",
    "group by caller_id\n",
    "having count(caller_id)>=3)\n",
    "select count(*) from result;\n",
    "'''\n",
    "\n",
    "calls_df=spark.read.format(\"csv\").option(\"inferSchema\", True).option(\"header\", True).load(\"calls1.csv\")\n",
    "users_df=spark.read.format(\"csv\").option(\"inferSchema\", True).option(\"header\", True).load(\"users1.csv\")\n",
    "\n",
    "join_df=calls_df.join(users_df, calls_df.caller_id==users_df.user_id, \"inner\")\n",
    "\n",
    "group_df=join_df.groupBy(\"caller_id\").count()\n",
    "\n",
    "filtered_df=group_df.filter(col(\"count\")>=3)\n",
    "\n",
    "filtered_df.count()\n",
    "\n",
    "\n",
    "'''\n",
    "Question 7:\n",
    "Convert below query into pyspark code:\n",
    "\n",
    "select rentalHistory.room_id, \n",
    "count(rentalHistory.room_id) as 'no_of_bookings',\n",
    "sum(rentalHistory.number_nights * rooms.price) as 'total_earnings' from rentalHistory\n",
    "inner join rooms on rooms.room_id=rentalHistory.room_id\n",
    "group by rentalHistory.room_id\n",
    "order by no_of_bookings desc ;\n",
    "'''\n",
    "\n",
    "rooms_df=spark.read.format(\"csv\").option(\"header\", True).load(\"rooms.csv\")\n",
    "hist_df=spark.read.format(\"csv\").option(\"header\", True).load(\"hist.csv\")\n",
    "\n",
    "join_df=hist_df.join(rooms_df, hist_df.room_id==rooms_df.room_id, \"inner\")\n",
    "\n",
    "result_df=join_df.groupBy(hist_df.room_id).agg(count(hist_df.room_id).alias(\"no_of_bookings\"), \n",
    "                                         sum(hist_df.number_nights*rooms_df.price).alias(\"total_earnings\")).orderBy(desc(\"no_of_bookings\"))\n",
    "result_df.show()\n",
    "\n",
    "\n",
    "'''\n",
    "Question 8:\n",
    "convert below query into pyspark code:\n",
    "\n",
    "select d.restaurant_name, sum(o.sales_amount) as sales from delivery_orders d\n",
    "inner join order_value o\n",
    "on \n",
    "d.delivery_id=o.delivery_id\n",
    "and d.actual_delivery_time is not null\n",
    "group by  d.restaurant_name\n",
    "order by sales desc\n",
    "limit 2;\n",
    "'''\n",
    "\n",
    "delivery_df=spark.read.format(\"csv\").option(\"header\", True).load(\"delivery.csv\")\n",
    "order_df=spark.read.format(\"csv\").option(\"header\", True).load(\"order.csv\")\n",
    "\n",
    "result_df=delivery_df.join(order_df, delivery_df.delivery_id==order_df.delivery_id, \"inner\")\\\n",
    ".filter(order_df.actual_delivery_time.isNotNull())\\\n",
    ".groupBy(order_df.restaurant_name)\\\n",
    ".agg(sum(delivery_df.sales_amount).alias('sales'))\\\n",
    ".orderBy(desc('sales')).limit(2)\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "'''\n",
    "Question 9:\n",
    "convert below query into pyspark code:\n",
    "select driver_id, round(avg(order_total),3) as avg_total\n",
    "from delivery_details\n",
    "where datediff(minute,delivered_to_consumer_datetime,driver_at_restaurant_datetime )<=45\n",
    "group by driver_id\n",
    "having round(avg(order_total),3)>30\n",
    "order by avg_total desc;\n",
    "'''\n",
    "\n",
    "result_df=df1.withColumn(\"time_diff\", (unix_timestamp(\"delivered_to_consumer_datetime\")-unix_timestamp(\"driver_at_restaurant_datetime\"))/60)\\\n",
    ".filter(col(\"time_diff\")<=45)\\\n",
    ".groupBy(col(\"driver_id\"))\\\n",
    ".agg(round(avg(col(\"order_total\")),3))\\\n",
    ".alias(\"avg_total\")\\\n",
    ".filter(col(\"avg_total\")>30)\\\n",
    ".orderBy(desc(col(\"avg_total\")))\\\n",
    ".show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 10:\n",
    "convert below query into pyspark code:\n",
    "select shipment_id, weight\n",
    "from \n",
    "(select shipment_id, weight, dense_rank() over(order by weight desc) as ranking from shipments) as a\n",
    "where ranking=3\n",
    "\n",
    "'''\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"shipments.csv\")\n",
    "window=Window.orderBy(col(\"weight\").desc())\n",
    "df1=df.withColumn(\"ranking\", dense_rank().over(window))\n",
    "df2=df1.filter(df1.ranking==3)\n",
    "df1.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 11:\n",
    "convert below query to pyspark code:\n",
    "\n",
    "with cte as (\n",
    "select *, rank() over (partition by company order by year) as rank_by_year,\n",
    "rank() over (partition by company order by revenue) as rank_by_revenue,\n",
    "(cast(rank() over (partition by company order by year) as signed)\n",
    " - cast(rank() over (partition by company order by revenue)as signed)) as diff\n",
    "from company_revenue\n",
    ")\n",
    "\n",
    "select company from cte\n",
    "group by company\n",
    "having count(distinct diff)=1 and avg(diff)=0;\n",
    "'''\n",
    "\n",
    "company_df=spark.read.format('csv').option('header', True).load(\"company_revenue.csv\")\n",
    "\n",
    "window=Window.partitionBy(col(\"company\")).orderBy(col(\"year\"))\n",
    "window2=Window.partitionBy(col(\"company\")).orderBy(col(\"revenue\"))\n",
    "\n",
    "result_df=company_df.withColumn(\"rank_by_year\", rank().over(window))\\\n",
    ".withColumn(\"rank_by_revenue\", rank().over(window2))\\\n",
    ".withColumn(\"diff\", rank().over(window)- rank().over(window2))\\\n",
    "\n",
    "final_df=result_df.groupBy(col(\"company\"))\\\n",
    ".agg(countDistinct(\"diff\").alias(\"distinct_diff_count\"), avg(\"diff\").alias(\"avg_diff\"))\\\n",
    ".filter((col(\"distinct_diff_count\")==1) & (col(\"avg_diff\")==0))\n",
    "\n",
    "final_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 12:\n",
    "convert below query into pyspark code:\n",
    "\n",
    "with cte as (\n",
    "select *,\n",
    "row_number() over (partition by subj_1, subj_2 order by marks) as ranks\n",
    "from my_table1\n",
    ")\n",
    "\n",
    "select id, subj_1, subj_2, marks from cte where ranks<=1;\n",
    "\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"my_table1.csv\")\n",
    "window=Window.partitionBy(\"subj_1\", \"subj_2\").orderBy(\"marks\")\n",
    "\n",
    "result_df=df.withColumn(\"ranks\", row_number().over(window))\n",
    "\n",
    "final_df=result_df.filter(col(\"ranks\")<=1)\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "final_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 13:\n",
    "convert below query into pyspark code:\n",
    "\n",
    " with cte as(\n",
    " select sname, marks, row_number() over (partition by sname order by marks desc) as result\n",
    " from students1\n",
    ")\n",
    "\n",
    "select sname, sum(marks) as total from cte\n",
    "where result<=2\n",
    "group by sname\n",
    "\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"students_1.csv\")\n",
    "\n",
    "window=Window.partitionBy(\"sname\").orderBy(desc(\"marks\"))\n",
    "\n",
    "result_df=df.withColumn(\"results\", row_number().over(window))\n",
    "\n",
    "final_df=result_df.filter(result_df.results<=2)\\\n",
    ".groupBy(\"sname\")\\\n",
    ".agg(sum(result_df.marks).alias(\"total\"))\n",
    "\n",
    "final_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 14:\n",
    "convert below query into pyspark code:\n",
    "\n",
    "with cte as(\n",
    "select *, row_number() over(partition by subjects order by marks) as result\n",
    "from student_marks\n",
    ")\n",
    "select student_id, student_name, subjects, marks from cte\n",
    "where result<=2;\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"student_marks.csv\")\n",
    "\n",
    "#df.show()\n",
    "window=Window.partitionBy(\"subjects\").orderBy(\"marks\")\n",
    "\n",
    "result_df=df.withColumn(\"result\", row_number().over(window))\n",
    "\n",
    "#result_df.show()\n",
    "\n",
    "final_df=result_df.filter(col(\"result\")<=2).select(\"student_id\",\"student_name\",\"subjects\",\"marks\")\n",
    "\n",
    "final_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 15:\n",
    "Calculate average marks, count of students for each subject\n",
    "\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"student_marks.csv\")\n",
    "\n",
    "result_df=df.groupBy(\"subjects\")\\\n",
    ".agg(avg(\"marks\").alias(\"avg_marks\"), count(\"student_name\").alias(\"count\"))\\\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 16:\n",
    "Calculate Running total\n",
    "\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"student_marks.csv\")\n",
    "\n",
    "window=Window.partitionBy(\"subjects\").orderBy(\"student_id\")\n",
    "\n",
    "result_df=df.withColumn(\"result\", sum(col(\"marks\")).over(window))\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 17:\n",
    "To calculate the percentage of total salary that each employee contributes to their respective department.\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"student_marks.csv\")\n",
    "\n",
    "#df.show()\n",
    "\n",
    "df1=df.groupBy(\"subjects\").agg(sum(\"marks\").alias(\"total_marks\"))\n",
    "\n",
    "#df1.show()\n",
    "\n",
    "join_df=df.join(df1, df.subjects==df1.subjects)\n",
    "\n",
    "join_df.show()\n",
    "\n",
    "result_df=join_df.withColumn(\"percentage\", round((col(\"marks\")/col(\"total_marks\"))*100, 2))\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 18:\n",
    "Calculate Running average\n",
    "\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"student_marks.csv\")\n",
    "\n",
    "window=Window.partitionBy(\"subjects\").orderBy(\"student_id\")\n",
    "\n",
    "result_df=df.withColumn(\"average\", avg(col(\"marks\")).over(window))\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 19:\n",
    "convert below query into pyspark code:\n",
    "\n",
    "with cte as (\n",
    "select  product_category, avg(price) as average, max(price) as maximum\n",
    "from products2\n",
    "group by product_category\n",
    ")\n",
    "\n",
    "select a.*, c.* from products2 a join cte c \n",
    "on a.product_category=c.product_category\n",
    "and a.price>c.average\n",
    "and a.price<c.maximum;\n",
    "\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"products2.csv\")\n",
    "\n",
    "window=Window.partitionBy(\"product_category\")\n",
    "result_df=df.withColumn(\"average\", avg(col(\"price\")).over(window)).withColumn(\"maximum\", max(col(\"price\")).over(window))\\\n",
    "\n",
    "join_df=df.join(result_df, df.product_category==result_df.product_category ,\"inner\")\\\n",
    ".filter((df.price>result_df.average) & (df.price<result_df.maximum))\n",
    "\n",
    "join_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 20:\n",
    "convert below query into pyspark code:\n",
    "\n",
    "with cte as (\n",
    "select *, datediff(day, order_date, date()) as diff\n",
    "from orders4\n",
    ")\n",
    "\n",
    "select customer_id from cte where diff>7 and diff<=30;\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"orders4.csv\")\n",
    "\n",
    "result_df=df.withColumn(\"diff\", datediff(current_date(), col(\"order_date\")))\n",
    "\n",
    "final_df=result_df.filter((col(\"diff\")>7) & (col(\"diff\")<=30))\n",
    "\n",
    "final_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 21:\n",
    "convert below query into pyspark code:\n",
    "\n",
    "with cte as (\n",
    "select *, avg(salary) over (partition by dept_id) as average from employee1\n",
    ")\n",
    "\n",
    "select emp_id , dept_id, name , salary, average  from cte\n",
    "where salary>average\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"employee1.csv\")\n",
    "window=Window.partitionBy(\"dept_id\")\n",
    "\n",
    "result_df=df.withColumn(\"average\", avg(\"salary\").over(window))\n",
    "\n",
    "final_df=result_df.filter(col(\"salary\")>col(\"average\")).select(\"emp_id\", \"name\", \"dept_id\", \"salary\")\n",
    "\n",
    "final_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 22:\n",
    "\n",
    "data=[(\"1\", \"abc\", \"5647 7463 7678 8625\"), (\"2\", \"xyz\",\"7987 7867 7862 7353\")]\n",
    "\n",
    "column_names=[\"id\", \"name\", \"card\"]\n",
    "\n",
    "df=spark.createDataFrame(data, column_names)\n",
    "\n",
    "Given above example, mask name column values as from the list \n",
    "for example, mask \"5647 7463 7678 8625\" with \"**** **** **** 8625\" and \"7987 7867 7862 7353\" with \"**** **** **** 8625\"\n",
    "'''\n",
    "\n",
    "result_df=df.withColumn(\"name\", regexp_replace(col(\"name\"), r\"\\d{4} \\d{4} \\d{4}\", \"**** **** ***\"))\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 23:\n",
    "\n",
    "How to check Spark UI through Jupyter notebook:\n",
    "'''\n",
    "\n",
    "import pyspark, os, sys\n",
    "from pyspark.sql import *\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import types\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "spark=SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark.sparkContext.uiWebUrl    #with the help of this command, we can go to spark UI to monitor \n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 24:\n",
    "Given a large dataset that doesn’t fit in memory, how would you convert a Pandas DataFrame to a PySpark DataFrame for scalable processing?\n",
    "'''\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "spark=SparkSession.builder.getOrCreate()\n",
    "\n",
    "chunksize=10**6   #here we have specified chunksize as 1 million per chunk\n",
    "\n",
    "df=pd.read_csv(\"large_dataset.csv\", chunksize=chunksize)\n",
    "\n",
    "spark_df=spark.createDataFrame(df)\n",
    "\n",
    "spark_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 25:\n",
    "Given the CustomerOrders table, \n",
    "Determine the percentage of unpaid orders per region, calculated as the number of unpaid orders divided by the total number of orders in that region.\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"customerOrders.csv\")\n",
    "df1=df.filter(col(\"PaymentStatus\")==\"Unpaid\").groupBy(\"Region\").count()\n",
    "\n",
    "df2=df.groupBy(\"Region\").agg(count(\"OrderID\").alias(\"cnt\"))\n",
    "\n",
    "final_df=df1.join(df2, df1.Region==df2.Region, \"inner\").withColumn(\"Percentage\", round((df1[\"count\"]/df2[\"cnt\"])*100,2))\n",
    "\n",
    "result_df=final_df.orderBy(col(\"Percentage\").desc())\n",
    "\n",
    "result_df.select(df1[\"Region\"], \"Percentage\").show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 26:\n",
    "Convert below query into pyspark code:\n",
    "\n",
    "with cte as (\n",
    "select *, row_number() over (partition by state order by population desc) as max_pop,\n",
    "row_number() over (partition by state order by population) as min_pop from city_population\n",
    ")\n",
    "\n",
    "select c.* from cte c where c.max_pop=1 or c.min_pop=1\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"city_population.csv\")\n",
    "\n",
    "window1=Window.partitionBy(\"state\").orderBy(desc(\"population\"))\n",
    "\n",
    "window2=Window.partitionBy(\"state\").orderBy(\"population\")\n",
    "\n",
    "result_df=df.withColumn(\"max_pop\", row_number().over(window1)).withColumn(\"min_pop\", row_number().over(window2))\n",
    "\n",
    "final_df=result_df.filter((result_df.max_pop==1) | (result_df.min_pop==1))\n",
    "\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b5030c1-d3d0-4037-8005-efebdb4d3a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-------+------+-----------------+\n",
      "|emp_id|   name|dept_id|salary|           result|\n",
      "+------+-------+-------+------+-----------------+\n",
      "|     1|  Alice|    101| 60000|          60000.0|\n",
      "|    10|   Judy|    104| 72000|          66000.0|\n",
      "|    11|Mallory|    104| 71000|67666.66666666667|\n",
      "|    12|   Niaj|    104| 73000|          69000.0|\n",
      "|    13| Olivia|    105| 82000|          71600.0|\n",
      "|    14|  Peggy|    105| 80000|          73000.0|\n",
      "|    15|  Sybil|    105| 78000|73714.28571428571|\n",
      "|    16|  Trent|    106| 90000|          75750.0|\n",
      "|    17| Victor|    106| 88000|77111.11111111111|\n",
      "|    18| Walter|    106| 87000|          78100.0|\n",
      "|    19| Xander|    107| 67000|77090.90909090909|\n",
      "|     2|    Bob|    101| 70000|          76500.0|\n",
      "|    20| Yvonne|    107| 69000|75923.07692307692|\n",
      "|     3|Charlie|    101| 55000|74428.57142857143|\n",
      "|     4|  David|    102| 80000|          74800.0|\n",
      "|     5|    Eve|    102| 85000|          75437.5|\n",
      "|     6|  Frank|    102| 75000|75411.76470588235|\n",
      "|     7|  Grace|    103| 65000|74833.33333333333|\n",
      "|     8|  Heidi|    103| 68000|74473.68421052632|\n",
      "|     9|   Ivan|    103| 70000|          74250.0|\n",
      "+------+-------+-------+------+-----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'http://DESKTOP-84AAC5E:4040'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark, os, sys\n",
    "from pyspark.sql import *\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import types\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "spark=SparkSession.builder.getOrCreate()\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"employee1.csv\")\n",
    "window=Window.partitionBy(\"dept_id\")\n",
    "\n",
    "w1=Window.orderBy(\"emp_id\")\n",
    "result_df=df.withColumn(\"average\", avg(\"salary\").over(window))\n",
    "\n",
    "#df.show()\n",
    "res_df=df.withColumn(\"result\", avg(\"salary\").over(w1))\n",
    "res_df.orderBy(\"emp_id\").show()\n",
    "\n",
    "spark.sparkContext.uiWebUrl    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2fb7973-a5cb-4270-817c-a4e2ebcbdc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+----------+-------+-------+\n",
      "|      state|     city|population|max_pop|min_pop|\n",
      "+-----------+---------+----------+-------+-------+\n",
      "|    Haryana|   Ambala|       100|      4|      1|\n",
      "|    Haryana|  Gurgaon|       300|      1|      4|\n",
      "|  Karnataka|Mangalore|       200|      3|      1|\n",
      "|  Karnataka|Bangalore|       900|      1|      3|\n",
      "|Maharashtra|   Mumbai|      1000|      4|      1|\n",
      "|Maharashtra|     Pune|       600|      1|      4|\n",
      "|     Punjab| Amritsar|       150|      4|      1|\n",
      "|     Punjab| Ludhiana|       400|      1|      4|\n",
      "+-----------+---------+----------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark, os, sys\n",
    "from pyspark.sql import *\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import types\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "spark=SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4491b08-d9da-472e-8a09-a593fe14398f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
