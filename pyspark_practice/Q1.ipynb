{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea18e291-1395-420b-b843-0739d9b74ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-------------+\n",
      "|   City|grouped_city|activity_rank|\n",
      "+-------+------------+-------------+\n",
      "| Mumbai|           2|            1|\n",
      "|Newyork|           2|            2|\n",
      "| Nashik|           1|            3|\n",
      "+-------+------------+-------------+\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `user_id` cannot be resolved. Did you mean one of the following? [`City`, `Id`, `State`, `County`, `Zipcode`].;\n'Aggregate ['user_id], ['user_id, sum('listen_duration) AS total_listening#93, 'count(distinct 'song_id) AS distinct_songs#94]\n+- Relation [Id#17,Institution_Name#18,Branch_Name#19,Branch_Number#20,City#21,County#22,State#23,Zipcode#24,2010_Deposits#25,2011_Deposits#26,2012_Deposits#27,2013_Deposits#28,2014_Deposits#29,2015_Deposits#30,2016_Deposits#31] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 41\u001b[0m\n\u001b[0;32m     30\u001b[0m df_final\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03mQuestion 2:\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124;03mConvert below query to pyspark code:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03mgroup by user_id;\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m df_group\u001b[38;5;241m=\u001b[39m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlisten_duration\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtotal_listening\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcountDistinct\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msong_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdistinct_songs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m df_final\u001b[38;5;241m=\u001b[39mdf_group\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_listening\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mround\u001b[39m(df_group\u001b[38;5;241m.\u001b[39mtotal_listening\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m))\n\u001b[0;32m     43\u001b[0m df_final\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\pythonlab\\Lib\\site-packages\\pyspark\\sql\\group.py:186\u001b[0m, in \u001b[0;36mGroupedData.agg\u001b[1;34m(self, *exprs)\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(c, Column) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m exprs), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall exprs should be Column\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    185\u001b[0m     exprs \u001b[38;5;241m=\u001b[39m cast(Tuple[Column, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m], exprs)\n\u001b[1;32m--> 186\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jgd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession)\n",
      "File \u001b[1;32m~\\pythonlab\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\pythonlab\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `user_id` cannot be resolved. Did you mean one of the following? [`City`, `Id`, `State`, `County`, `Zipcode`].;\n'Aggregate ['user_id], ['user_id, sum('listen_duration) AS total_listening#93, 'count(distinct 'song_id) AS distinct_songs#94]\n+- Relation [Id#17,Institution_Name#18,Branch_Name#19,Branch_Number#20,City#21,County#22,State#23,Zipcode#24,2010_Deposits#25,2011_Deposits#26,2012_Deposits#27,2013_Deposits#28,2014_Deposits#29,2015_Deposits#30,2016_Deposits#31] csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''Question. 1\n",
    "Covert below query to Pyspark code:\n",
    "\n",
    "select City , count(City) as grouped_city, \n",
    "row_number() over (order by count(City) desc) activity_rank \n",
    "from table \n",
    "group by City;\n",
    "'''\n",
    "\n",
    "import pyspark, os, sys\n",
    "from pyspark.sql import *\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import types\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "spark=SparkSession.builder.getOrCreate()\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"inferSchema\", True).option(\"header\", True).load(\"sample1.csv\")\n",
    "#df.show()\n",
    "\n",
    "\n",
    "df_1=df.select(\"City\").groupBy(\"City\").agg(count(\"City\").alias(\"grouped_city\"))\n",
    "#df_1.show()\n",
    "\n",
    "window_df=Window.orderBy(col(\"grouped_city\").desc())\n",
    "df_final=df_1.withColumn(\"activity_rank\", row_number().over(window_df))\n",
    "df_final.show()\n",
    "\n",
    "'''\n",
    "Question 2:\n",
    "Convert below query to pyspark code:\n",
    "\n",
    "select user_id, round(sum(listen_duration)/60) as 'total_listening', count(distinct(song_id)) as 'distinct_songs'\n",
    "from listening_habits\n",
    "group by user_id;\n",
    "'''\n",
    "\n",
    "df_group=df.groupBy(\"user_id\").agg(sum(\"listen_duration\").alias(\"total_listening\"),countDistinct(\"song_id\").alias(\"distinct_songs\"))\n",
    "df_final=df_group.withColumn(\"total_listening\", round(df_group.total_listening/60))\n",
    "df_final.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 3:\n",
    "Convert below query to pyspark code:\n",
    "\n",
    "with cte as\n",
    "(\n",
    "select *, lead(ID) over (order by ID) as next_id,\n",
    "lag(ID) over (order by ID) as prev_id\n",
    "from Students  \n",
    ") \n",
    "select case\n",
    "when ID%2!=0 and next_id is not null then next_id\n",
    "when ID%2=0 then prev_id\n",
    "when ID%2!=0 and next_id is null then ID\n",
    "end\n",
    "as ID, name\n",
    "from cte;\n",
    "'''\n",
    "\n",
    "window_df=Window.orderBy(col('ID'))\n",
    "df=spark.read.format(\"csv\").option(\"inferSchema\", True).option(\"header\", True).load(\"example_student.csv\")\n",
    "\n",
    "result_df=df.withColumn(\"next_id\", lead(\"ID\").over(window_df)).withColumn(\"prev_id\", lag(\"ID\").over(window_df))\n",
    "\n",
    "df1=result_df.withColumn(\"result_id\", when((df.ID%2!=0) & (result_df.next_id.isNotNull()), result_df.next_id)\n",
    "                                        .when(df.ID%2==0, result_df.prev_id)\n",
    "                                        .when((df.ID%2!=0) & (result_df.next_id.isNull()), df.ID))\n",
    "\n",
    "df1.select(\"Name\", \"result_id\").show()\n",
    "\n",
    "\n",
    "'''\n",
    "Question 4:\n",
    "Convert below query to pyspark code:\n",
    "\n",
    "select employees.id, employees.name from\n",
    "employees left join terminations\n",
    "on employees.id=terminations.emp_id\n",
    "where (terminations.term_date is null or terminations.term_date>'2016-01-02')\n",
    "order by employees.hire_date;\n",
    "'''\n",
    "\n",
    "emp_df=spark.read.format(\"csv\").option(\"header\", True).option(\"inferSchema\", True).load(\"emp1.csv\")\n",
    "term_df=spark.read.format(\"csv\").option(\"header\", True).option(\"inferSchema\", True).load(\"term1.csv\")\n",
    "result_df=emp_df.join(term_df, emp_df.id==term_df.emp_id, \"left\").filter((col(\"term_date\").isNull()) | (col(\"term_date\")> '2016-01-02')).orderBy(\"hire_date\")\n",
    "\n",
    "result_df.select(\"id\", \"name\").show()\n",
    "\n",
    "\n",
    "'''\n",
    "Question 5:\n",
    "Convert below query into pyspark code:\n",
    "\n",
    "select * from customers\n",
    "where id not in \n",
    "(\n",
    "select customers.id from customers inner join orders1\n",
    "on customers.id = orders1.customerId\n",
    ")\n",
    "order by customers.name;\n",
    "'''\n",
    "\n",
    "cust_df=spark.read.format(\"csv\").option(\"inferSchema\", True).option(\"header\", True).load(\"cust1.csv\")\n",
    "order_df=spark.read.format(\"csv\").option(\"inferSchema\", True).option(\"header\", True).load(\"order1.csv\")\n",
    "\n",
    "join_df=cust_df.join(order_df, cust_df.id==order_df.customerId, \"inner\")\n",
    "\n",
    "cust_with_order_df=join_df.select(cust_df[\"id\"])\n",
    "\n",
    "filtered_customer_df= cust_df.filter(~cust_df[\"id\"].isin([row.id for row in cust_with_order_df.collect()]))\n",
    "\n",
    "result_df=filtered_customer_df.orderBy(\"name\")\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "\n",
    "'''\n",
    "Question 6:\n",
    "Convert below query into pyspark code:\n",
    "\n",
    "with result as (\n",
    "select distinct caller_id, count(caller_id)\n",
    "from calls c inner join users u\n",
    "on c.caller_id=u.user_id\n",
    "group by caller_id\n",
    "having count(caller_id)>=3)\n",
    "select count(*) from result;\n",
    "'''\n",
    "\n",
    "calls_df=spark.read.format(\"csv\").option(\"inferSchema\", True).option(\"header\", True).load(\"calls1.csv\")\n",
    "users_df=spark.read.format(\"csv\").option(\"inferSchema\", True).option(\"header\", True).load(\"users1.csv\")\n",
    "\n",
    "join_df=calls_df.join(users_df, calls_df.caller_id==users_df.user_id, \"inner\")\n",
    "\n",
    "group_df=join_df.groupBy(\"caller_id\").count()\n",
    "\n",
    "filtered_df=group_df.filter(col(\"count\")>=3)\n",
    "\n",
    "filtered_df.count()\n",
    "\n",
    "\n",
    "'''\n",
    "Question 7:\n",
    "Convert below query into pyspark code:\n",
    "\n",
    "select rentalHistory.room_id, \n",
    "count(rentalHistory.room_id) as 'no_of_bookings',\n",
    "sum(rentalHistory.number_nights * rooms.price) as 'total_earnings' from rentalHistory\n",
    "inner join rooms on rooms.room_id=rentalHistory.room_id\n",
    "group by rentalHistory.room_id\n",
    "order by no_of_bookings desc ;\n",
    "'''\n",
    "\n",
    "rooms_df=spark.read.format(\"csv\").option(\"header\", True).load(\"rooms.csv\")\n",
    "hist_df=spark.read.format(\"csv\").option(\"header\", True).load(\"hist.csv\")\n",
    "\n",
    "join_df=hist_df.join(rooms_df, hist_df.room_id==rooms_df.room_id, \"inner\")\n",
    "\n",
    "result_df=join_df.groupBy(hist_df.room_id).agg(count(hist_df.room_id).alias(\"no_of_bookings\"), \n",
    "                                         sum(hist_df.number_nights*rooms_df.price).alias(\"total_earnings\")).orderBy(desc(\"no_of_bookings\"))\n",
    "result_df.show()\n",
    "\n",
    "\n",
    "'''\n",
    "Question 8:\n",
    "convert below query into pyspark code:\n",
    "\n",
    "select d.restaurant_name, sum(o.sales_amount) as sales from delivery_orders d\n",
    "inner join order_value o\n",
    "on \n",
    "d.delivery_id=o.delivery_id\n",
    "and d.actual_delivery_time is not null\n",
    "group by  d.restaurant_name\n",
    "order by sales desc\n",
    "limit 2;\n",
    "'''\n",
    "\n",
    "delivery_df=spark.read.format(\"csv\").option(\"header\", True).load(\"delivery.csv\")\n",
    "order_df=spark.read.format(\"csv\").option(\"header\", True).load(\"order.csv\")\n",
    "\n",
    "result_df=delivery_df.join(order_df, delivery_df.delivery_id==order_df.delivery_id, \"inner\")\\\n",
    ".filter(delivery_df.actual_delivery_time.isNotNull())\\\n",
    ".groupBy(order_df.restaurant_name)\\\n",
    ".agg(sum(delivery_df.sales_amount).alias('sales'))\\\n",
    ".orderBy(desc('sales')).limit(2)\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "'''\n",
    "Question 9:\n",
    "convert below query into pyspark code:\n",
    "select driver_id, round(avg(order_total),3) as avg_total\n",
    "from delivery_details\n",
    "where datediff(minute,delivered_to_consumer_datetime,driver_at_restaurant_datetime )<=45\n",
    "group by driver_id\n",
    "having round(avg(order_total),3)>30\n",
    "order by avg_total desc;\n",
    "'''\n",
    "\n",
    "result_df=df1.withColumn(\"time_diff\", (unix_timestamp(\"delivered_to_consumer_datetime\")-unix_timestamp(\"driver_at_restaurant_datetime\"))/60)\\\n",
    ".filter(col(\"time_diff\")<=45)\\\n",
    ".groupBy(col(\"driver_id\"))\\\n",
    ".agg(round(avg(col(\"order_total\")),3))\\\n",
    ".alias(\"avg_total\")\\\n",
    ".filter(col(\"avg_total\")>30)\\\n",
    ".orderBy(desc(col(\"avg_total\")))\\\n",
    ".show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 10:\n",
    "convert below query into pyspark code:\n",
    "select shipment_id, weight\n",
    "from \n",
    "(select shipment_id, weight, dense_rank() over(order by weight desc) as ranking from shipments) as a\n",
    "where ranking=3\n",
    "\n",
    "'''\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"shipments.csv\")\n",
    "window=Window.orderBy(col(\"weight\").desc())\n",
    "df1=df.withColumn(\"ranking\", dense_rank().over(window))\n",
    "df2=df1.filter(df1.ranking==3)\n",
    "df1.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 11:\n",
    "convert below query to pyspark code:\n",
    "\n",
    "with cte as (\n",
    "select *, rank() over (partition by company order by year) as rank_by_year,\n",
    "rank() over (partition by company order by revenue) as rank_by_revenue,\n",
    "(cast(rank() over (partition by company order by year) as signed)\n",
    " - cast(rank() over (partition by company order by revenue)as signed)) as diff\n",
    "from company_revenue\n",
    ")\n",
    "\n",
    "select company from cte\n",
    "group by company\n",
    "having count(distinct diff)=1 and avg(diff)=0;\n",
    "'''\n",
    "\n",
    "company_df=spark.read.format('csv').option('header', True).load(\"company_revenue.csv\")\n",
    "\n",
    "window=Window.partitionBy(col(\"company\")).orderBy(col(\"year\"))\n",
    "window2=Window.partitionBy(col(\"company\")).orderBy(col(\"revenue\"))\n",
    "\n",
    "result_df=company_df.withColumn(\"rank_by_year\", rank().over(window))\\\n",
    ".withColumn(\"rank_by_revenue\", rank().over(window2))\\\n",
    ".withColumn(\"diff\", rank().over(window)- rank().over(window2))\\\n",
    "\n",
    "final_df=result_df.groupBy(col(\"company\"))\\\n",
    ".agg(countDistinct(\"diff\").alias(\"distinct_diff_count\"), avg(\"diff\").alias(\"avg_diff\"))\\\n",
    ".filter((col(\"distinct_diff_count\")==1) & (col(\"avg_diff\")==0))\n",
    "\n",
    "final_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 12:\n",
    "convert below query into pyspark code:\n",
    "\n",
    "with cte as (\n",
    "select *,\n",
    "row_number() over (partition by subj_1, subj_2 order by marks) as ranks\n",
    "from my_table1\n",
    ")\n",
    "\n",
    "select id, subj_1, subj_2, marks from cte where ranks<=1;\n",
    "\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"my_table1.csv\")\n",
    "window=Window.partitionBy(\"subj_1\", \"subj_2\").orderBy(\"marks\")\n",
    "\n",
    "result_df=df.withColumn(\"ranks\", row_number().over(window))\n",
    "\n",
    "final_df=result_df.filter(col(\"ranks\")<=1)\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "final_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 13:\n",
    "convert below query into pyspark code:\n",
    "\n",
    " with cte as(\n",
    " select sname, marks, row_number() over (partition by sname order by marks desc) as result\n",
    " from students1\n",
    ")\n",
    "\n",
    "select sname, sum(marks) as total from cte\n",
    "where result<=2\n",
    "group by sname\n",
    "\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"students_1.csv\")\n",
    "\n",
    "window=Window.partitionBy(\"sname\").orderBy(desc(\"marks\"))\n",
    "\n",
    "result_df=df.withColumn(\"results\", row_number().over(window))\n",
    "\n",
    "final_df=result_df.filter(result_df.results<=2)\\\n",
    ".groupBy(\"sname\")\\\n",
    ".agg(sum(result_df.marks).alias(\"total\"))\n",
    "\n",
    "final_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 14:\n",
    "convert below query into pyspark code:\n",
    "\n",
    "with cte as(\n",
    "select *, row_number() over(partition by subjects order by marks) as result\n",
    "from student_marks\n",
    ")\n",
    "select student_id, student_name, subjects, marks from cte\n",
    "where result<=2;\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"student_marks.csv\")\n",
    "\n",
    "#df.show()\n",
    "window=Window.partitionBy(\"subjects\").orderBy(\"marks\")\n",
    "\n",
    "result_df=df.withColumn(\"result\", row_number().over(window))\n",
    "\n",
    "#result_df.show()\n",
    "\n",
    "final_df=result_df.filter(col(\"result\")<=2).select(\"student_id\",\"student_name\",\"subjects\",\"marks\")\n",
    "\n",
    "final_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 15:\n",
    "Calculate average marks, count of students for each subject\n",
    "\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"student_marks.csv\")\n",
    "\n",
    "result_df=df.groupBy(\"subjects\")\\\n",
    ".agg(avg(\"marks\").alias(\"avg_marks\"), count(\"student_name\").alias(\"count\"))\\\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 16:\n",
    "Calculate Running total\n",
    "\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"student_marks.csv\")\n",
    "\n",
    "window=Window.partitionBy(\"subjects\").orderBy(\"student_id\")\n",
    "\n",
    "result_df=df.withColumn(\"result\", sum(col(\"marks\")).over(window))\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 17:\n",
    "To calculate the percentage of total salary that each employee contributes to their respective department.\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"student_marks.csv\")\n",
    "\n",
    "#df.show()\n",
    "\n",
    "df1=df.groupBy(\"subjects\").agg(sum(\"marks\").alias(\"total_marks\"))\n",
    "\n",
    "#df1.show()\n",
    "\n",
    "join_df=df.join(df1, df.subjects==df1.subjects)\n",
    "\n",
    "join_df.show()\n",
    "\n",
    "result_df=join_df.withColumn(\"percentage\", round((col(\"marks\")/col(\"total_marks\"))*100, 2))\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 18:\n",
    "Calculate Running average\n",
    "\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"student_marks.csv\")\n",
    "\n",
    "window=Window.partitionBy(\"subjects\").orderBy(\"student_id\")\n",
    "\n",
    "result_df=df.withColumn(\"average\", avg(col(\"marks\")).over(window))\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 19:\n",
    "convert below query into pyspark code:\n",
    "\n",
    "with cte as (\n",
    "select  product_category, avg(price) as average, max(price) as maximum\n",
    "from products2\n",
    "group by product_category\n",
    ")\n",
    "\n",
    "select a.*, c.* from products2 a join cte c \n",
    "on a.product_category=c.product_category\n",
    "and a.price>c.average\n",
    "and a.price<c.maximum;\n",
    "\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"products2.csv\")\n",
    "\n",
    "window=Window.partitionBy(\"product_category\")\n",
    "result_df=df.withColumn(\"average\", avg(col(\"price\")).over(window)).withColumn(\"maximum\", max(col(\"price\")).over(window))\n",
    "\n",
    "result_df=result_df.withColumnRenamed(\"price\", \"cost\")\n",
    "join_df=df.join(result_df, df.product_category==result_df.product_category ,\"inner\")\\\n",
    ".filter((df.price>result_df.average) & (df.price<result_df.maximum))\n",
    "\n",
    "join_df.show()\n",
    "\n",
    "\n",
    "'''\n",
    "Question 20:\n",
    "convert below query into pyspark code:\n",
    "\n",
    "with cte as (\n",
    "select *, datediff(day, order_date, date()) as diff\n",
    "from orders4\n",
    ")\n",
    "\n",
    "select customer_id from cte where diff>7 and diff<=30;\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"orders4.csv\")\n",
    "\n",
    "result_df=df.withColumn(\"diff\", datediff(current_date(), col(\"order_date\")))\n",
    "\n",
    "final_df=result_df.filter((col(\"diff\")>7) & (col(\"diff\")<=30))\n",
    "\n",
    "final_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 21:\n",
    "convert below query into pyspark code:\n",
    "\n",
    "with cte as (\n",
    "select *, avg(salary) over (partition by dept_id) as average from employee1\n",
    ")\n",
    "\n",
    "select emp_id , dept_id, name , salary, average  from cte\n",
    "where salary>average\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"employee1.csv\")\n",
    "window=Window.partitionBy(\"dept_id\")\n",
    "\n",
    "result_df=df.withColumn(\"average\", avg(\"salary\").over(window))\n",
    "\n",
    "final_df=result_df.filter(col(\"salary\")>col(\"average\")).select(\"emp_id\", \"name\", \"dept_id\", \"salary\")\n",
    "\n",
    "final_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 22:\n",
    "\n",
    "data=[(\"1\", \"abc\", \"5647 7463 7678 8625\"), (\"2\", \"xyz\",\"7987 7867 7862 7353\")]\n",
    "\n",
    "column_names=[\"id\", \"name\", \"card\"]\n",
    "\n",
    "df=spark.createDataFrame(data, column_names)\n",
    "\n",
    "Given above example, mask name column values as from the list \n",
    "for example, mask \"5647 7463 7678 8625\" with \"**** **** **** 8625\" and \"7987 7867 7862 7353\" with \"**** **** **** 8625\"\n",
    "'''\n",
    "\n",
    "result_df=df.withColumn(\"name\", regexp_replace(col(\"name\"), r\"\\d{4} \\d{4} \\d{4}\", \"**** **** ***\"))\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 23:\n",
    "\n",
    "How to check Spark UI through Jupyter notebook:\n",
    "'''\n",
    "\n",
    "import pyspark, os, sys\n",
    "from pyspark.sql import *\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import types\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "spark=SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark.sparkContext.uiWebUrl    #with the help of this command, we can go to spark UI to monitor \n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 24:\n",
    "Given a large dataset that doesn’t fit in memory, how would you convert a Pandas DataFrame to a PySpark DataFrame for scalable processing?\n",
    "'''\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "\n",
    "spark=SparkSession.builder.getOrCreate()\n",
    "\n",
    "chunksize=10**6   #here we have specified chunksize as 1 million per chunk\n",
    "\n",
    "df=pd.read_csv(\"large_dataset.csv\", chunksize=chunksize)\n",
    "\n",
    "spark_df=spark.createDataFrame(df)\n",
    "\n",
    "spark_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 25:\n",
    "Given the CustomerOrders table, \n",
    "Determine the percentage of unpaid orders per region, calculated as the number of unpaid orders divided by the total number of orders in that region.\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"customerOrders.csv\")\n",
    "df1=df.filter(col(\"PaymentStatus\")==\"Unpaid\").groupBy(\"Region\").count()\n",
    "\n",
    "df2=df.groupBy(\"Region\").agg(count(\"OrderID\").alias(\"cnt\"))\n",
    "\n",
    "final_df=df1.join(df2, df1.Region==df2.Region, \"inner\").withColumn(\"Percentage\", round((df1[\"count\"]/df2[\"cnt\"])*100,2))\n",
    "\n",
    "result_df=final_df.orderBy(col(\"Percentage\").desc())\n",
    "\n",
    "result_df.select(df1[\"Region\"], \"Percentage\").show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 26:\n",
    "Convert below query into pyspark code:\n",
    "\n",
    "with cte as (\n",
    "select *, row_number() over (partition by state order by population desc) as max_pop,\n",
    "row_number() over (partition by state order by population) as min_pop from city_population\n",
    ")\n",
    "\n",
    "select c.* from cte c where c.max_pop=1 or c.min_pop=1\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"city_population.csv\")\n",
    "\n",
    "window1=Window.partitionBy(\"state\").orderBy(desc(\"population\"))\n",
    "\n",
    "window2=Window.partitionBy(\"state\").orderBy(\"population\")\n",
    "\n",
    "result_df=df.withColumn(\"max_pop\", row_number().over(window1)).withColumn(\"min_pop\", row_number().over(window2))\n",
    "\n",
    "final_df=result_df.filter((result_df.max_pop==1) | (result_df.min_pop==1))\n",
    "\n",
    "final_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 27:\n",
    "Convert below query into pyspark code:\n",
    "\n",
    "with cte as (\n",
    "select month(orderdate) as mn, avg(sales_amount) as avg_sales from salesdata group by mn\n",
    ")\n",
    "\n",
    "select c.*, a.productid from cte c join salesdata a on c.mn=month(a.orderdate) and a.sales_amount>c.avg_sales\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"salesdata.csv\")\n",
    "\n",
    "window=Window.partitionBy(month(\"orderdate\"))\n",
    "\n",
    "df1=df.withColumn(\"avg_sales\", avg(\"sales_amount\").over(window))\n",
    "\n",
    "result_df=df1.filter(col(\"sales_amount\")>col(\"avg_sales\"))\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 28:\n",
    "Convert below query into pyspark code:\n",
    "\n",
    "with cte as (\n",
    "select *, instr(email, '@') as position from new_customer\n",
    ")\n",
    "select count(customer_id) as cnt, substr(email, position, length(email)-1) as domain \n",
    " from cte group by substr(email, position, length(email)-1) order by cnt desc\n",
    "\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"new_customer.csv\")\n",
    "\n",
    "df1=df.withColumn(\"position\", instr(df.email, \"@\"))\n",
    "\n",
    "df2=df1.withColumn(\"domain\", substr(df.email, df1.position, length(df.email)-1))\n",
    "\n",
    "final_df=df2.groupBy(\"domain\").agg(count(df.customer_id).alias(\"cnt\")).orderBy(desc(count(df.customer_id)))\n",
    "\n",
    "final_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 29:\n",
    "Convert below query into pyspark code:\n",
    "\n",
    "with cte as(\n",
    "select *, quantity*unit_price as sales, row_number() over (partition by region order by (quantity*unit_price) desc) as result,\n",
    "sum(quantity*unit_price) over (partition by region) as total_sales from saledata\n",
    ")\n",
    "\n",
    "select *, (sales/total_sales)*100 as percentage from cte where result=1 and total_sales>500\n",
    "\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"sale_data.csv\")\n",
    "window=Window.partitionBy(\"region\").orderBy(desc(col(\"quantity\")*col(\"unit_price\")))\n",
    "\n",
    "window2=Window.partitionBy(\"region\")\n",
    "result_df=df.withColumn(\"sales\", col(\"quantity\")*col(\"unit_price\"))\n",
    "\n",
    "df1=result_df.withColumn(\"result\", row_number().over(window))\\\n",
    ".withColumn(\"total_sales\", sum(\"sales\").over(window2))\\\n",
    ".withColumn(\"percentage\", (col(\"sales\")/col(\"total_sales\"))*100)\\\n",
    ".filter((col(\"result\")==1) & (col(\"total_sales\")>500))\n",
    "\n",
    "df1.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 30:\n",
    "Convert below query into pyspark code:\n",
    "\n",
    "select \n",
    "case when rating between 8.0 and 8.9 then \"8.0-8.9\" \n",
    "when rating between 6.0 and 6.9 then \"6.0-6.9\"\n",
    "when rating between 7.0 and 7.9 then \"7.0-7.9\"\n",
    "when rating between 9.0 and 9.9 then \"9.0-9.9\"\n",
    "when rating =10.0 then \"perfect 10\"\n",
    "else  \"not recommended\"\n",
    "end as ranges,\n",
    "count(*) as counts\n",
    "from MovieData\n",
    "group by ranges;\n",
    "\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"movie_data.csv\")\n",
    "\n",
    "df_temp=df.withColumn(\"rating_range\", when((df.rating>=8.0) & (df.rating<=8.9), \"8.0-8.9\")\\\n",
    "                     .when((df.rating>=9.0) & (df.rating<=9.9), \"9.0-9.9\")\\\n",
    "                     .when((df.rating>=7.0) & (df.rating<=7.9), \"7.0-7.9\")\\\n",
    "                     .when((df.rating>=6.0) & (df.rating<=6.9), \"6.0-6.9\")\\\n",
    "                     .when(df.rating==10.0, \"perfect_10\")\\\n",
    "                     .otherwise(\"not recommended\"))\n",
    "\n",
    "result_df=df_temp.groupBy(\"rating_range\").count()\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 31:\n",
    "Convert below query into pyspark code:\n",
    "\n",
    "with cte as (select *, month(dates) as mn from revenue_table)\n",
    "select *, sum(revenue) over(partition by mn order by dates desc) as total from cte \n",
    "'''\n",
    "\n",
    "r_df=spark.read.format(\"csv\").option(\"header\", True).load(\"revenue_table.csv\")\n",
    "\n",
    "df=r_df.withColumn(\"mn\", month(\"dates\"))\n",
    "\n",
    "window=Window.partitionBy(\"mn\").orderBy(desc(\"dates\"))\n",
    "\n",
    "result_df=df.withColumn(\"total\", sum(\"revenue\").over(window))\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 32:\n",
    "Convert below query into pyspark code:\n",
    "\n",
    "with cte as (\n",
    "select * from flights where departure_date=actual_departure_date\n",
    ")\n",
    "select count(c.flight_id ) as on_time_departures,  count(*) as total_flights from flights left join cte c\n",
    "on flights.flight_id=c.flight_id\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"flights.csv\")\n",
    "\n",
    "window=Window.orderBy(\"flight_id\")\n",
    "\n",
    "filter_df=df.filter(df.departure_date==df.actual_departure_date)\n",
    "\n",
    "join_df=df.join(filter_df, df.flight_id==filter_df.flight_id, \"left\")\n",
    "\n",
    "result_df=df.withColumn(\"total_flights\", count(df.flight_id).over(window))\n",
    "\n",
    "final_df=filter_df.withColumn(\"on_time_departure\", count(filter_df.flight_id).over(window))\n",
    "\n",
    "result_df.orderBy(desc(\"total_flights\")).show(1)\n",
    "\n",
    "final_df.orderBy(desc(\"on_time_departure\")).show(1)\n",
    "\n",
    "final_join=result_df.join(final_df, result_df.flight_id==final_df.flight_id, \"left\")\n",
    "\n",
    "final_join.agg(max(\"total_flights\").alias(\"total_flights\"), max(\"on_time_departure\").alias(\"on_time_departure\")).show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 33:\n",
    "Convert below query into pyspark code:\n",
    "\n",
    "select city, \n",
    "count(*) as Total_Count,\n",
    "count(case when gender='M' then 1 end) as Male_count,\n",
    "count(case when gender='F' then 1 end) as Female_count,\n",
    "sum(case when gender='M' then salary end) as Total_Male_salary,\n",
    "sum(case when gender='F' then salary end) as Total_Female_salary\n",
    "from cities group by city\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"cities.csv\")\n",
    "\n",
    "result_df=df.groupBy(\"city\").agg(count(\"city\").alias(\"total_count\"), \n",
    "                                 count(when(df.gender==\"M\", 1)).alias(\"Male_Count\"),\n",
    "                                 count(when(df.gender==\"F\", 1)).alias(\"Female_count\"),\n",
    "                                 sum(when(df.gender==\"M\", df.salary)).alias(\"Total_Male_Salary\"),\n",
    "                                 sum(when(df.gender==\"F\", df.salary)).alias(\"Total_female_Salary\"))\n",
    "result_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 34:\n",
    "\n",
    "You are given with the dataset below, get the RESULTANT dataset using PYSPARK:\n",
    "\n",
    "Input dataset:\n",
    "+----------+------------+\n",
    "|Department|EmployeeName|\n",
    "+----------+------------+\n",
    "|    IT|    John|\n",
    "|    IT|    Jane|\n",
    "|    IT|   Robert|\n",
    "|    IT|    Emily|\n",
    "|    HR|    Mike|\n",
    "|    HR|    Alice|\n",
    "|    HR|   Sophia|\n",
    "|    HR|   Daniel|\n",
    "|  Finance|   Olivia|\n",
    "|  Finance|    Liam|\n",
    "| Marketing|    Emma|\n",
    "| Marketing|    Noah|\n",
    "+----------+------------+\n",
    "\n",
    "Resultant dataset:\n",
    "+---------+---------------------------------+\n",
    "|     dept|              e_list         |cnt|\n",
    "+---------+-----------------------------+---+\n",
    "|       HR|[Mike, Alice, Sophia, Daniel]|  4|\n",
    "|  Finance|      [Olivia, Liam]         |  2|\n",
    "|Marketing|        [Emma, Noah]         |  2|\n",
    "|       IT|[John, Jane, Robert, Emily]  |  4|\n",
    "+---------+-----------------------------+---+\n",
    "'''\n",
    "\n",
    "spark=SparkSession.builder.getOrCreate()\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"emp_1.csv\")\n",
    "\n",
    "df.show()\n",
    "\n",
    "result_df=df.groupBy(\"dept\").agg(collect_list(\"name\").alias(\"e_list\"),count(\"dept\").alias(\"cnt\"))\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 35:\n",
    "You are given with the dataset below, Identify rows containing non-numeric values in the \"Quantity\" column, if any using pyspark.\n",
    "\n",
    "Input data:\n",
    "+-----------+--------------+---------+------+\n",
    "|productCode|      Quantity|unitPrice|custId|\n",
    "+-----------+--------------+---------+------+\n",
    "|       P001|             5|       20|  C001|\n",
    "|       P002|             &|     15.5|  C002|\n",
    "|       P003|          10-n|     5.99|  C003|\n",
    "|       P004|             2|    18.98|  C001|\n",
    "|       P005|         eight|    12.75|  C002|\n",
    "|       P006|seven and half|   118.75|  C006|\n",
    "+-----------+--------------+---------+------+\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"store.csv\")\n",
    "\n",
    "df.show()\n",
    "\n",
    "result_df=df.filter(col(\"Quantity\").rlike('[^0-9.]'))\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 36:\n",
    "Convert null values with specific values in below data:\n",
    "+----+--------+----+\n",
    "|  id|    name| age|\n",
    "+----+--------+----+\n",
    "|   1|Van Dijk|  23|\n",
    "|   2|    NULL|  32|\n",
    "|   3|   Messi|NULL|\n",
    "|   4|    NULL|NULL|\n",
    "|   5|    Kaka|NULL|\n",
    "|   6| Ronaldo|  35|\n",
    "|   7|    NULL|  28|\n",
    "|   8|   Messi|NULL|\n",
    "|NULL|  Neymar|  29|\n",
    "|  10|  Mbappe|  22|\n",
    "+----+--------+----+\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"person.csv\")\n",
    "\n",
    "result_df=df.na.fill({\"id\":5, \"name\":\"messi\", \"age\":28})\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 37:\n",
    "Convert below query into pyspark code:\n",
    "\n",
    "with cte as (\n",
    "select *, row_number() over(partition by id order by marks desc) as n from students_new  \n",
    "),\n",
    "\n",
    "cte2 as (\n",
    "select * from cte where n<=3\n",
    ")\n",
    "select id, avg(marks) from cte2 group by id\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"students_new.csv\")\n",
    "\n",
    "window=Window.partitionBy(\"id\").orderBy(desc(\"marks\"))\n",
    "\n",
    "window2=Window.partitionBy(\"id\")\n",
    "\n",
    "df1=df.withColumn(\"n\", row_number().over(window))\n",
    "\n",
    "df2=df1.filter(df1.n<=3)\n",
    "\n",
    "result_df=df2.withColumn(\"average\", avg(\"marks\").over(window2))\n",
    "\n",
    "result_df.select(\"id\", \"average\").distinct().show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 38:\n",
    "Convert below query into pyspark code:\n",
    "\n",
    "select a.student_id from StudentSubjects a join StudentSubjects b on\n",
    "a.subject=\"Chemistry\" and b.subject=\"Physics\" and\n",
    "a.student_id=b.student_id and\n",
    "a.marks=b.marks;\n",
    "'''\n",
    "\n",
    "df1=spark.read.format(\"csv\").option(\"header\", True).load(\"StudentSubjects.csv\")\n",
    "\n",
    "df2=spark.read.format(\"csv\").option(\"header\", True).load(\"StudentSubjects.csv\")\n",
    "\n",
    "r_df=df1.join(df2, (df1.student_id==df2.student_id) & (df1.marks==df2.marks), \"inner\").filter((df1.subject==\"Chemistry\") & (df2.subject==\"Physics\")).select(df1.student_id)\n",
    "\n",
    "r_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Qustion 39:\n",
    "Convert below query into pyspark code:\n",
    "\n",
    "with cte as (\n",
    "select CUSTOMER_ID, SUM(REVENUE) as total_revenue from CustomerPurchases group by CUSTOMER_ID\t\n",
    "),\n",
    "cte2 as (\n",
    "select CUSTOMER_ID, REVENUE  from CustomerPurchases where PRODUCT=\"Photoshop\"\n",
    ")\n",
    "select a.CUSTOMER_ID, a.total_revenue-b.REVENUE as final_revenue from cte a join cte2 b on a.CUSTOMER_ID=b.CUSTOMER_ID\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"CustomerPurchases.csv\")\n",
    "window=Window.partitionBy(\"cust_id\")\n",
    "\n",
    "df1=df.withColumn(\"total_revenue\", sum(\"revenue\").over(window)).withColumnRenamed(\"cust_id\", \"customer_id\")\n",
    "\n",
    "df2=df1.select(\"customer_id\", \"total_revenue\").distinct()\n",
    "\n",
    "df3=df.filter(col(\"product\")==\"Photoshop\")\n",
    "\n",
    "join_df=df2.join(df3, df2.customer_id==df3.cust_id, \"inner\")\n",
    "\n",
    "final_df=join_df.withColumn(\"final_revenue\", join_df.total_revenue-join_df.revenue)\n",
    "\n",
    "final_df.select(\"customer_id\", \"final_revenue\").show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 40:\n",
    "Convert below query into pyspark code:\n",
    "\n",
    "with cte as (\n",
    "select a.product_id, a.name, a.category_id, avg(b.rating) as average_rating, \n",
    "dense_rank() over (partition by a.category_id order by avg(b.rating) desc) as category_rank \n",
    "from Products a join Reviews b on a.product_id=b.product_id\n",
    "group by a.product_id, a.name, a.category_id\n",
    ")\n",
    "\n",
    "select product_id, name, category_id, average_rating from cte where category_rank<=3;\n",
    "\n",
    "'''\n",
    "\n",
    "df1=spark.read.format(\"csv\").option(\"header\", True).load(\"product.csv\")\n",
    "\n",
    "temp_df=df1.withColumnRenamed(\"product_id\", \"pid\")\n",
    "\n",
    "df2=spark.read.format(\"csv\").option(\"header\", True).load(\"reviews.csv\")\n",
    "\n",
    "join_df=temp_df.join(df2, temp_df.pid==df2.product_id, \"inner\")\n",
    "\n",
    "result_df=join_df.groupBy(\"pid\", \"name \", \"category\").agg(avg(\"rating\").alias(\"average_rating\"))\n",
    "\n",
    "window=Window.partitionBy(\"category\").orderBy(desc(\"average_rating\"))\n",
    "\n",
    "final_df=result_df.withColumn(\"category_rank\", dense_rank().over(window))\n",
    "\n",
    "final=final_df.filter(col(\"category_rank\")<=3)\n",
    "\n",
    "final.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 41:\n",
    "Convert below query into pyspark code:\n",
    "\n",
    "with cte as(\n",
    "select *, row_number() over (partition by id order by order_date) as result from orders\n",
    "),\n",
    "cte2 as (\n",
    "select *, lead(order_date) over(partition by id order by result) as next_date from cte\n",
    ")\n",
    "select id from cte2 where next_date-order_date=1;\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"orders1.csv\")\n",
    "\n",
    "window=Window.partitionBy(\"id\").orderBy(\"order_date\")\n",
    "window2=Window.partitionBy(\"id\").orderBy(\"result\")\n",
    "\n",
    "df1=df.withColumn(\"result\", row_number().over(window))\n",
    "\n",
    "df2=df1.withColumn(\"next_date\", lead(\"order_date\").over(window2))\n",
    "\n",
    "final_df=df2.withColumn(\"diff\", datediff(col(\"next_date\"), col(\"order_date\")))\n",
    "\n",
    "final_df.filter(col(\"diff\")==1).select(\"id\").show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 42:\n",
    "explode() in pyspark\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"city.csv\")\n",
    "\n",
    "result_df=df.withColumn(\"locations\", split(\"location\", \",\"))\n",
    "\n",
    "result_df=result_df.withColumn(\"results\", explode(\"locations\"))\n",
    "result_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 43:\n",
    "bucketBy() in pyspark\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"CustomerPurchases.csv\")\n",
    "\n",
    "df.show()\n",
    "\n",
    "df3=df.write.bucketBy(2, \"product\").saveAsTable(\"bucketed_table1\")\n",
    "\n",
    "result_df=spark.read.table(\"bucketed_table1\")\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 44:\n",
    "Convert below query into pyspark code:\n",
    "\n",
    "with cte as(\n",
    "select *, row_number() over (partition by city) as city_count from city_gender\n",
    "),\n",
    "cte2 as (\n",
    "select city, gender, max(city_count) over (partition by city) as max_count from cte\n",
    "),\n",
    "cte3 as (\n",
    "select *, count(gender) over (partition by city) as male_count from cte2 where gender='M'\n",
    "),\n",
    "cte4 as(\n",
    "select *, count(gender) over (partition by city) as female_count from cte2 where gender='F'\n",
    ")\n",
    "select distinct a.city, a.max_count, a.male_count, b.female_count from cte3 a left join cte4 b on a.city=b.city;\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"city_gender.csv\")\n",
    "\n",
    "window=Window.partitionBy(\"city\").orderBy(\"city\")\n",
    "\n",
    "result_df=df.withColumn(\"city_count\", row_number().over(window))\n",
    "\n",
    "result_df=result_df.withColumn(\"max_count\", max(\"city_count\").over(window))\n",
    "\n",
    "df1=result_df.withColumn(\"male_count\", count(when(col(\"gender\")=='M', 1)\\\n",
    "                                                .otherwise(None)).over(window))\n",
    "\n",
    "df2=result_df.withColumn(\"female_count\", count(when(col(\"gender\")=='F', 1)\\\n",
    "                                                  .otherwise(None)).over(window))\n",
    "\n",
    "join_df=df1.join(df2, df1.city==df2.city, \"left\")\n",
    "\n",
    "join_df.select(df1.city, df1.max_count, df1.male_count, df2.female_count).distinct().show()\n",
    "\n",
    "\n",
    "'''\n",
    "Question 45:\n",
    "split() function in pyspark:\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"marks.csv\")\n",
    "\n",
    "result_df=df.withColumn(\"marks\", split(\"marks\", \"\\\\|\"))\n",
    "\n",
    "result_df=result_df.withColumn(\"physics\", result_df.marks[0])\\\n",
    ".withColumn(\"chemistry\", result_df.marks[1])\\\n",
    ".withColumn(\"maths\", result_df.marks[2])\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 46:\n",
    "For below data, replace non-numeric values with empty values and calculate age of customers by calculating difference between registered date & current date:\n",
    "+-----------+---------------+--------------------+-------------+----------+\n",
    "|customer_id|           name|               email|        phone|  reg_date|\n",
    "+-----------+---------------+--------------------+-------------+----------+\n",
    "|          1|       John Doe|  john.doe@gmail.com| 123-456-7890|2022-01-15|\n",
    "|          2|     Jane Smith|jane.smith@hotmai...|(987)654-3210|2021-11-30|\n",
    "|          3|      Alice Lee| alice.lee@yahoo.com|     555-5555|2023-03-10|\n",
    "|          4|      Bob Brown| bob.brown@gmail.com|         NULL|2022-05-20|\n",
    "|          5|Charlie Johnson|charlie.johnson@g...|     789-0123|2021-08-12|\n",
    "|          6|      Eva Davis| eva.davis@yahoo.com|(123)456-7890|2022-09-18|\n",
    "|          7|   Frank Wilson|frank.wilson@hotm...|     111-2222|2023-01-05|\n",
    "+-----------+---------------+--------------------+-------------+----------+\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"customer.csv\")\n",
    "\n",
    "result_df=df.withColumn(\"phone\", regexp_replace(col(\"phone\"), '[^0-9]', \"\"))  #replaced non-numeric values with empty space\n",
    "\n",
    "result_df=result_df.withColumn(\"reg_date\", to_date(result_df.reg_date, \"yyyy-mm-dd\"))\n",
    "\n",
    "result_df=result_df.withColumnRenamed(\"name\", \"Full name\")\\\n",
    "                    .withColumn(\"current_date\", current_date())\\\n",
    "                    .withColumn(\"age\", (current_date()-col(\"reg_date\")).cast(IntegerType()))\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 45:\n",
    "Remove duplicates from list\n",
    "lists=[1,2,3,4,5,5,2]\n",
    "'''\n",
    "\n",
    "df = spark.read.format(\"csv\").option(\"header\", True).load(\"lists.csv\")\n",
    "\n",
    "window=Window.partitionBy(\"result\").orderBy(\"result\")\n",
    "\n",
    "df=df.withColumn(\"splits\", split(\"lists\", \",\"))\n",
    "\n",
    "df=df.withColumn(\"result\", explode(\"splits\"))\n",
    "\n",
    "df=df.withColumn(\"ranks\", row_number().over(window))\n",
    "\n",
    "df=df.filter(col(\"ranks\")==1)\n",
    "\n",
    "df=df.select(\"result\")\n",
    "\n",
    "result_df=df.agg(collect_list(\"result\").alias(\"final\"))\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 46:\n",
    "1.to get current date\n",
    "2.no. of days difference between current date and joining date\n",
    "3.no. of months difference between current date and joining date\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"items.csv\")\n",
    "df.show()\n",
    "df1=df.withColumn(\"current\", current_date())  #to get current date\n",
    "df1.show()\n",
    "df2=df1.withColumn(\"joining\", lit(\"2024-12-01\").cast(\"date\"))\n",
    "df2.show()\n",
    "df3=df2.withColumn(\"diff\", datediff(col(\"current\"), col(\"joining\")))  #no. of days difference between current date and joining date\n",
    "df3.show()\n",
    "df4=df3.withColumn(\"month_diff\", months_between(col(\"current\"), col(\"joining\")))   #no. of months difference between current date and joining date\n",
    "df4.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 47:\n",
    "Swapping items with lead() and lag()\n",
    "'''\n",
    "\n",
    "window=Window.orderBy(\"order_id\")\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"items_order.csv\")\n",
    "df2=df.withColumn(\"correct_ordered_items\", when(col(\"order_id\")%2!=0, coalesce(lead(\"item\").over(window), col(\"item\"))).otherwise(lag(\"item\").over(window)))  \n",
    "#It takes the next row's \"item\" value (or the current \"item\" value if there's no next row).\n",
    "df2.show()\n",
    "\n",
    "\n",
    "'''\n",
    "Question 48:\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "041c870e-7faf-4028-bec4-545e50b7f294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----+------+\n",
      "|emp_id|   name| age|salary|\n",
      "+------+-------+----+------+\n",
      "|     1|  alice|  25|  5000|\n",
      "|     2|    bob|  30|  6000|\n",
      "|     3|charlie|30.0|  5500|\n",
      "|     4|  david|  35|5500.0|\n",
      "|     5|    eve|30.0|5500.0|\n",
      "+------+-------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark, os, sys\n",
    "from pyspark.sql import *\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import types\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "spark=SparkSession.builder.getOrCreate()\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"employees.csv\")\n",
    "#df.show()\n",
    "\n",
    "result_df=df.agg(avg(\"age\").alias(\"average_age\"), avg(\"salary\").alias(\"average_salary\")).collect()[0]\n",
    "avg_df=result_df.average_age\n",
    "avg_sal=result_df.average_salary\n",
    "\n",
    "result_df=df.na.fill({\"age\": avg_df, \"salary\": avg_sal})\n",
    "\n",
    "result_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c223967e-df4b-40ab-a70e-a519b636cea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----+--------+---------+-------------------+-----------+-----+-------+-------------+--------------------+-------------------+----------+\n",
      "|trade_id|stock_id|price|quantity|   status|         trade_date|total_sales|ranks|user_id|         city|               email|          join_date|city_count|\n",
      "+--------+--------+-----+--------+---------+-------------------+-----------+-----+-------+-------------+--------------------+-------------------+----------+\n",
      "|  100264|     148|  4.8|      40|Completed|2022-08-26 12:00:00|      192.0|    1|    148|       Boston|sailor9820@gmail.com|2021-08-20 12:00:00|         1|\n",
      "|  100259|     148|  5.1|      35|Completed|2022-08-25 12:00:00|      178.5|    2|    148|       Boston|sailor9820@gmail.com|2021-08-20 12:00:00|         1|\n",
      "|  100305|     300|   10|      15|Completed|2022-09-05 12:00:00|      150.0|    3|    300|San Francisco|houstoncowboy1122...|2022-06-30 12:00:00|         3|\n",
      "|  100400|     178|  9.9|      15|Completed|2022-09-09 12:00:00|      148.5|    4|    178|San Francisco|harrypotterfan182...|2022-01-05 12:00:00|         3|\n",
      "|  100565|     265| 25.6|       5|Completed|2022-12-19 12:00:00|      128.0|    5|    265|       Denver|shadower_@hotmail...|2022-02-26 12:00:00|         1|\n",
      "|  100102|     111|   10|      10|Completed|2022-08-17 12:00:00|      100.0|    6|    111|San Francisco|    rrokl0@gmail.com|2021-08-03 12:00:00|         3|\n",
      "+--------+--------+-----+--------+---------+-------------------+-----------+-----+-------+-------------+--------------------+-------------------+----------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[city: string, count: bigint]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark, os, sys\n",
    "from pyspark.sql import *\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import types\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql.functions import instr\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "\n",
    "spark=SparkSession.builder.getOrCreate()\n",
    "\n",
    "window=Window.orderBy(desc(\"total_sales\"))\n",
    "\n",
    "window2=Window.partitionBy(\"city\")\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"trades.csv\")\n",
    "\n",
    "df2=spark.read.format(\"csv\").option(\"header\", True).load(\"users.csv\")\n",
    "\n",
    "df=df.withColumn(\"total_sales\", col(\"quantity\")*col(\"price\"))\n",
    "\n",
    "df=df.withColumn(\"ranks\", row_number().over(window))\n",
    "\n",
    "df=df.filter(col(\"status\")=='Completed')\n",
    "\n",
    "df2=df2.withColumn(\"city_count\", count(\"city\").over(window2))\n",
    "\n",
    "#df2.show()\n",
    "\n",
    "join_df=df.join(df2, df.stock_id==df2.user_id, \"inner\")\n",
    "\n",
    "join_df.show()\n",
    "\n",
    "join_df.groupBy(\"city\").count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ee2285df-beca-4474-b5c2-0fd320391e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|      name|  result|\n",
      "+----------+--------+\n",
      "|20/04/1963|20041963|\n",
      "|21/04/1964|21041964|\n",
      "|23/04/1962|23041962|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark, os, sys\n",
    "from pyspark.sql import *\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import types\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql.functions import instr\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "\n",
    "spark=SparkSession.builder.getOrCreate()\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"temp_3.csv\")\n",
    "\n",
    "df1=df.withColumn(\"result\", regexp_replace(\"name\", \"[^0-9]\", \"\"))\n",
    "\n",
    "df1=df1.withColumn(\"result\", df1.result.cast(\"Integer\"))\n",
    "\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cc5afcba-9ead-4cba-81e4-0bf40a94ccff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+------+-----+\n",
      "|          lists|              splits|result|ranks|\n",
      "+---------------+--------------------+------+-----+\n",
      "|1,2,3,4,5,5,6,2|[1, 2, 3, 4, 5, 5...|     1|    1|\n",
      "|1,2,3,4,5,5,6,2|[1, 2, 3, 4, 5, 5...|     2|    1|\n",
      "|1,2,3,4,5,5,6,2|[1, 2, 3, 4, 5, 5...|     3|    1|\n",
      "|1,2,3,4,5,5,6,2|[1, 2, 3, 4, 5, 5...|     4|    1|\n",
      "|1,2,3,4,5,5,6,2|[1, 2, 3, 4, 5, 5...|     5|    1|\n",
      "|1,2,3,4,5,5,6,2|[1, 2, 3, 4, 5, 5...|     6|    1|\n",
      "+---------------+--------------------+------+-----+\n",
      "\n",
      "+------------------+\n",
      "|             final|\n",
      "+------------------+\n",
      "|[1, 2, 3, 4, 5, 6]|\n",
      "+------------------+\n",
      "\n",
      "+---+---+\n",
      "|max|min|\n",
      "+---+---+\n",
      "|  6|  1|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark, os, sys\n",
    "from pyspark.sql import *\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import types\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "spark=SparkSession.builder.getOrCreate()\n",
    "df = spark.read.format(\"csv\").option(\"header\", True).load(\"lists.csv\")\n",
    "\n",
    "window=Window.partitionBy(\"result\").orderBy(\"result\")\n",
    "\n",
    "df=df.withColumn(\"splits\", split(\"lists\", \",\"))\n",
    "\n",
    "df=df.withColumn(\"result\", explode(\"splits\"))\n",
    "\n",
    "df=df.withColumn(\"ranks\", row_number().over(window))\n",
    "\n",
    "df=df.filter(col(\"ranks\")==1)\n",
    "df.show()\n",
    "df=df.select(\"result\")\n",
    "\n",
    "result_df=df.agg(collect_list(\"result\").alias(\"final\"))\n",
    "result_df.show()\n",
    "\n",
    "df=df.agg(max(\"result\").alias(\"max\"), min(\"result\").alias(\"min\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35e54536-db77-43c8-8266-95fa0828ad3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------+-------+\n",
      "|items|price|current|joining|\n",
      "+-----+-----+-------+-------+\n",
      "+-----+-----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark, os, sys\n",
    "from pyspark.sql import *\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import types\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql.functions import instr\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "\n",
    "spark=SparkSession.builder.getOrCreate()\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"items.csv\")\n",
    "#df.show()\n",
    "df1=df.withColumn(\"current\", current_date())  #to get current date\n",
    "#df1.show()\n",
    "df2=df1.withColumn(\"joining\", lit(\"2024-12-01\").cast(\"date\"))\n",
    "#df2.show()\n",
    "df3=df2.filter(col(\"joining\").between(\"2024-11-01\", \"2024-11-16\"))\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2661419a-a6cb-4920-a9bc-d43e5c9c9635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+------------+\n",
      "|customer_id|customer_name|product_name|\n",
      "+-----------+-------------+------------+\n",
      "|          1|       Daniel|           A|\n",
      "|          1|       Daniel|           B|\n",
      "|          1|       Daniel|           D|\n",
      "|          1|       Daniel|           C|\n",
      "|          2|        Diana|           A|\n",
      "|          3|    Elizabeth|           A|\n",
      "|          3|    Elizabeth|           B|\n",
      "|          3|    Elizabeth|           D|\n",
      "|          4|         John|           C|\n",
      "+-----------+-------------+------------+\n",
      "\n",
      "+-------------+-----------+------------+\n",
      "|customer_name|customer_id|        list|\n",
      "+-------------+-----------+------------+\n",
      "|         John|          4|         [C]|\n",
      "|       Daniel|          1|[A, B, D, C]|\n",
      "|        Diana|          2|         [A]|\n",
      "|    Elizabeth|          3|   [A, B, D]|\n",
      "+-------------+-----------+------------+\n",
      "\n",
      "+-------------+-----------+\n",
      "|customer_name|customer_id|\n",
      "+-------------+-----------+\n",
      "|    Elizabeth|          3|\n",
      "+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark, os, sys\n",
    "from pyspark.sql import *\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import types\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql.functions import instr\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.functions import col, array_contains\n",
    "\n",
    "\n",
    "spark=SparkSession.builder.getOrCreate()\n",
    "\n",
    "df1=spark.read.format(\"csv\").option(\"header\", True).load(\"cust.csv\")\n",
    "\n",
    "df2=spark.read.format(\"csv\").option(\"header\", True).load(\"ord.csv\")\n",
    "\n",
    "window=Window.partitionBy(\"customer_id\").orderBy(\"customer_id\")\n",
    "\n",
    "df2=df2.withColumn(\"ranks\", row_number().over(window))\n",
    "\n",
    "join_df=df1.join(df2, df1.customer_id==df2.customer_id, \"inner\").select(df1.customer_id, df1.customer_name, df2.product_name)\n",
    "join_df.show()\n",
    "\n",
    "join_df=join_df.groupBy(\"customer_name\", \"customer_id\").agg(collect_list(\"product_name\").alias(\"list\"))\n",
    "join_df.show()\n",
    "\n",
    "join_df=join_df.filter((~array_contains(col(\"list\"), 'C')) & (array_contains(col(\"list\"), 'A')) & (array_contains(col(\"list\"), 'B')) & (array_contains(col(\"list\"), 'D')))\n",
    "join_df.select(\"customer_name\", \"customer_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7226fe1a-f474-435e-a73c-d748876697fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+--------+---+\n",
      "|trans_is|user_id|tran_amt|avg|\n",
      "+--------+-------+--------+---+\n",
      "|       1|    101|     500|2.0|\n",
      "|       3|    101|     300|2.0|\n",
      "|       7|    101|     200|2.0|\n",
      "|       2|    102|     200|1.5|\n",
      "|       5|    102|     400|1.5|\n",
      "|       4|    103|     100|1.0|\n",
      "|       6|    103|     600|1.0|\n",
      "+--------+-------+--------+---+\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DataFrame.filter() missing 1 required positional argument: 'condition'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 36\u001b[0m\n\u001b[0;32m     33\u001b[0m result_df\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrans_is\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtran_amt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mavg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     34\u001b[0m result_df\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m---> 36\u001b[0m result_df\u001b[38;5;241m=\u001b[39m\u001b[43mresult_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: DataFrame.filter() missing 1 required positional argument: 'condition'"
     ]
    }
   ],
   "source": [
    "import pyspark, os, sys\n",
    "from pyspark.sql import *\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import types\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql.functions import instr\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.functions import col, array_contains\n",
    "\n",
    "\n",
    "spark=SparkSession.builder.getOrCreate()\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"tran.csv\")\n",
    "\n",
    "df=df.withColumn(\"tran_dt\", col(\"tran_dt\").cast(\"timestamp\"))\n",
    "\n",
    "window=Window.partitionBy(\"user_id\").orderBy(\"tran_dt\")\n",
    "\n",
    "df=df.withColumn(\"next_dt\", lead(\"tran_dt\").over(window))\n",
    "\n",
    "df=df.withColumn(\"diff\", datediff(\"next_dt\", \"tran_dt\"))\n",
    "\n",
    "window2=Window.partitionBy(\"user_id\")\n",
    "\n",
    "df=df.withColumn(\"cnt\", count(\"trans_is\").over(window2))\\\n",
    "    .withColumn(\"total\", sum(\"diff\").over(window2))\n",
    "\n",
    "df=df.withColumn(\"avg\", col(\"total\")/col(\"cnt\"))\n",
    "\n",
    "result_df=df.select(\"trans_is\", \"user_id\", \"tran_amt\", \"avg\")\n",
    "result_df.show()\n",
    "\n",
    "result_df=result_df.filter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7882ebc7-bd20-423b-830f-46c85046a226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unchanged\n",
      "+---+----+------+----------+----------+\n",
      "| id|name|salary|start_date|  end_date|\n",
      "+---+----+------+----------+----------+\n",
      "|  3|  cc|   300|2025-01-01|2999-12-31|\n",
      "+---+----+------+----------+----------+\n",
      "\n",
      "new\n",
      "+------+--------+----------+----------+--------+\n",
      "|id_new|name_new|salary_new|start_date|end_date|\n",
      "+------+--------+----------+----------+--------+\n",
      "+------+--------+----------+----------+--------+\n",
      "\n",
      "updated\n",
      "+---+----+------+----------+----------+\n",
      "| id|name|salary|start_date|  end_date|\n",
      "+---+----+------+----------+----------+\n",
      "|  1|  aa|   100|2025-01-01|2025-05-29|\n",
      "|  2|  bb|   200|2025-01-01|2025-05-29|\n",
      "+---+----+------+----------+----------+\n",
      "\n",
      "+------+--------+----------+----------+----------+\n",
      "|id_new|name_new|salary_new|start_date|  end_date|\n",
      "+------+--------+----------+----------+----------+\n",
      "|     1|     aaa|       100|2025-05-30|2999-12-31|\n",
      "|     2|     bbb|       200|2025-05-30|2999-12-31|\n",
      "+------+--------+----------+----------+----------+\n",
      "\n",
      "final SCD2 result:\n",
      "+---+----+------+----------+----------+\n",
      "| id|name|salary|start_date|  end_date|\n",
      "+---+----+------+----------+----------+\n",
      "|  3|  cc|   300|2025-01-01|2999-12-31|\n",
      "|  1|  aa|   100|2025-01-01|2025-05-29|\n",
      "|  2|  bb|   200|2025-01-01|2025-05-29|\n",
      "|  1| aaa|   100|2025-05-30|2999-12-31|\n",
      "|  2| bbb|   200|2025-05-30|2999-12-31|\n",
      "+---+----+------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark, os, sys\n",
    "from pyspark.sql import *\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import types\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.functions import broadcast\n",
    "from pyspark.sql.functions import instr\n",
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import regexp_extract\n",
    "from pyspark.sql.functions import posexplode\n",
    "from pyspark.sql.functions import spark_partition_id\n",
    "from pyspark.sql.functions import substring\n",
    "from pyspark.sql.functions import array_contains\n",
    "\n",
    "\n",
    "spark=SparkSession.builder.getOrCreate()\n",
    "\n",
    "old_df=spark.read.format(\"csv\").option(\"header\", True).load(\"sample_scd.csv\")\n",
    "\n",
    "new_df=spark.read.format(\"csv\").option(\"header\", True).load(\"sample_scd2.csv\")\n",
    "\n",
    "old_df=old_df.withColumn(\"start_date\", col(\"start_date\").cast(\"date\"))\\\n",
    "            .withColumn(\"end_date\", col(\"end_date\").cast(\"date\"))\\\n",
    "            .withColumn(\"salary\", col(\"salary\").cast(\"Integer\"))\n",
    "\n",
    "new_df=new_df.withColumn(\"salary\", col(\"salary\").cast(\"Integer\"))\n",
    "\n",
    "new_df=new_df.withColumnRenamed(\"id\", \"id_new\")\\\n",
    "            .withColumnRenamed(\"name\", \"name_new\")\\\n",
    "            .withColumnRenamed(\"salary\", \"salary_new\")\n",
    "\n",
    "old_df=old_df.withColumn(\"hash_old\", hash(\"id\", \"name\", \"salary\"))\n",
    "new_df=new_df.withColumn(\"hash_new\", hash(\"id_new\", \"name_new\", \"salary_new\"))\n",
    "\n",
    "join_df=old_df.join(new_df, old_df.id==new_df.id_new, \"full\")\n",
    "#join_df.show()\n",
    "\n",
    "#unchanged\n",
    "print(\"unchanged\")\n",
    "df1=join_df.filter(col(\"hash_new\")==col(\"hash_old\"))\n",
    "df1=df1.select(\"id\", \"name\", \"salary\", \"start_date\", \"end_date\")\n",
    "df1.show()\n",
    "\n",
    "#new\n",
    "print(\"new\")\n",
    "df2=join_df.filter(col(\"hash_old\").isNull())\n",
    "df2=df2.select(\"id_new\", \"name_new\", \"salary_new\")\\\n",
    "        .withColumn(\"start_date\", current_date())\\\n",
    "        .withColumn(\"end_date\", lit(\"2999-12-31\").cast(\"date\"))\n",
    "df2.show()\n",
    "\n",
    "#updated\n",
    "print(\"updated\")\n",
    "df3=join_df.filter(col(\"hash_old\")!=col(\"hash_new\"))\n",
    "df3_a=df3.select(\"id\", \"name\", \"salary\", \"start_date\", \"end_date\")\\\n",
    "        .withColumn(\"end_date\", current_date()-1)\n",
    "df3_a.show()\n",
    "\n",
    "df3_b=df3.select(\"id_new\", \"name_new\", \"salary_new\")\\\n",
    "        .withColumn(\"start_date\", current_date())\\\n",
    "        .withColumn(\"end_date\", lit(\"2999-12-31\").cast(\"date\"))\n",
    "df3_b.show()\n",
    "\n",
    "print(\"final SCD2 result:\")\n",
    "final_df=df1.union(df2)\\\n",
    "            .union(df3_a)\\\n",
    "            .union(df3_b)\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78466222-426c-408a-a8d4-93b43b4f0419",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
