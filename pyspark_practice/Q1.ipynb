{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea18e291-1395-420b-b843-0739d9b74ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-------------+\n",
      "|   City|grouped_city|activity_rank|\n",
      "+-------+------------+-------------+\n",
      "| Mumbai|           2|            1|\n",
      "|Newyork|           2|            2|\n",
      "| Nashik|           1|            3|\n",
      "+-------+------------+-------------+\n",
      "\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `user_id` cannot be resolved. Did you mean one of the following? [`City`, `Id`, `State`, `County`, `Zipcode`].;\n'Aggregate ['user_id], ['user_id, sum('listen_duration) AS total_listening#93, 'count(distinct 'song_id) AS distinct_songs#94]\n+- Relation [Id#17,Institution_Name#18,Branch_Name#19,Branch_Number#20,City#21,County#22,State#23,Zipcode#24,2010_Deposits#25,2011_Deposits#26,2012_Deposits#27,2013_Deposits#28,2014_Deposits#29,2015_Deposits#30,2016_Deposits#31] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 41\u001b[0m\n\u001b[0;32m     30\u001b[0m df_final\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03mQuestion 2:\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124;03mConvert below query to pyspark code:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03mgroup by user_id;\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m df_group\u001b[38;5;241m=\u001b[39m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlisten_duration\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtotal_listening\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcountDistinct\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msong_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdistinct_songs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m df_final\u001b[38;5;241m=\u001b[39mdf_group\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_listening\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mround\u001b[39m(df_group\u001b[38;5;241m.\u001b[39mtotal_listening\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m60\u001b[39m))\n\u001b[0;32m     43\u001b[0m df_final\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\pythonlab\\Lib\\site-packages\\pyspark\\sql\\group.py:186\u001b[0m, in \u001b[0;36mGroupedData.agg\u001b[1;34m(self, *exprs)\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(c, Column) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m exprs), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall exprs should be Column\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    185\u001b[0m     exprs \u001b[38;5;241m=\u001b[39m cast(Tuple[Column, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m], exprs)\n\u001b[1;32m--> 186\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jgd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession)\n",
      "File \u001b[1;32m~\\pythonlab\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\pythonlab\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `user_id` cannot be resolved. Did you mean one of the following? [`City`, `Id`, `State`, `County`, `Zipcode`].;\n'Aggregate ['user_id], ['user_id, sum('listen_duration) AS total_listening#93, 'count(distinct 'song_id) AS distinct_songs#94]\n+- Relation [Id#17,Institution_Name#18,Branch_Name#19,Branch_Number#20,City#21,County#22,State#23,Zipcode#24,2010_Deposits#25,2011_Deposits#26,2012_Deposits#27,2013_Deposits#28,2014_Deposits#29,2015_Deposits#30,2016_Deposits#31] csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''Question. 1\n",
    "Covert below query to Pyspark code:\n",
    "\n",
    "select City , count(City) as grouped_city, \n",
    "row_number() over (order by count(City) desc) activity_rank \n",
    "from table \n",
    "group by City;\n",
    "'''\n",
    "\n",
    "import pyspark, os, sys\n",
    "from pyspark.sql import *\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import types\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "spark=SparkSession.builder.getOrCreate()\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"inferSchema\", True).option(\"header\", True).load(\"sample1.csv\")\n",
    "#df.show()\n",
    "\n",
    "\n",
    "df_1=df.select(\"City\").groupBy(\"City\").agg(count(\"City\").alias(\"grouped_city\"))\n",
    "#df_1.show()\n",
    "\n",
    "window_df=Window.orderBy(col(\"grouped_city\").desc())\n",
    "df_final=df_1.withColumn(\"activity_rank\", row_number().over(window_df))\n",
    "df_final.show()\n",
    "\n",
    "'''\n",
    "Question 2:\n",
    "Convert below query to pyspark code:\n",
    "\n",
    "select user_id, round(sum(listen_duration)/60) as 'total_listening', count(distinct(song_id)) as 'distinct_songs'\n",
    "from listening_habits\n",
    "group by user_id;\n",
    "'''\n",
    "\n",
    "df_group=df.groupBy(\"user_id\").agg(sum(\"listen_duration\").alias(\"total_listening\"),countDistinct(\"song_id\").alias(\"distinct_songs\"))\n",
    "df_final=df_group.withColumn(\"total_listening\", round(df_group.total_listening/60))\n",
    "df_final.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 3:\n",
    "Convert below query to pyspark code:\n",
    "\n",
    "with cte as\n",
    "(\n",
    "select *, lead(ID) over (order by ID) as next_id,\n",
    "lag(ID) over (order by ID) as prev_id\n",
    "from Students  \n",
    ") \n",
    "select case\n",
    "when ID%2!=0 and next_id is not null then next_id\n",
    "when ID%2=0 then prev_id\n",
    "when ID%2!=0 and next_id is null then ID\n",
    "end\n",
    "as ID, name\n",
    "from cte;\n",
    "'''\n",
    "\n",
    "window_df=Window.orderBy(col('ID'))\n",
    "df=spark.read.format(\"csv\").option(\"inferSchema\", True).option(\"header\", True).load(\"example_student.csv\")\n",
    "\n",
    "result_df=df.withColumn(\"next_id\", lead(\"ID\").over(window_df)).withColumn(\"prev_id\", lag(\"ID\").over(window_df))\n",
    "\n",
    "df1=result_df.withColumn(\"result_id\", when((df.ID%2!=0) & (result_df.next_id.isNotNull()), result_df.next_id)\n",
    "                                        .when(df.ID%2==0, result_df.prev_id)\n",
    "                                        .when((df.ID%2!=0) & (result_df.next_id.isNull()), df.ID))\n",
    "\n",
    "df1.select(\"Name\", \"result_id\").show()\n",
    "\n",
    "\n",
    "'''\n",
    "Question 4:\n",
    "Convert below query to pyspark code:\n",
    "\n",
    "select employees.id, employees.name from\n",
    "employees left join terminations\n",
    "on employees.id=terminations.emp_id\n",
    "where (terminations.term_date is null or terminations.term_date>'2016-01-02')\n",
    "order by employees.hire_date;\n",
    "'''\n",
    "\n",
    "emp_df=spark.read.format(\"csv\").option(\"header\", True).option(\"inferSchema\", True).load(\"emp1.csv\")\n",
    "term_df=spark.read.format(\"csv\").option(\"header\", True).option(\"inferSchema\", True).load(\"term1.csv\")\n",
    "result_df=emp_df.join(term_df, emp_df.id==term_df.emp_id, \"left\").filter((col(\"term_date\").isNull()) | (col(\"term_date\")> '2016-01-02')).orderBy(\"hire_date\")\n",
    "\n",
    "result_df.select(\"id\", \"name\").show()\n",
    "\n",
    "\n",
    "'''\n",
    "Question 5:\n",
    "Convert below query into pyspark code:\n",
    "\n",
    "select * from customers\n",
    "where id not in \n",
    "(\n",
    "select customers.id from customers inner join orders1\n",
    "on customers.id = orders1.customerId\n",
    ")\n",
    "order by customers.name;\n",
    "'''\n",
    "\n",
    "cust_df=spark.read.format(\"csv\").option(\"inferSchema\", True).option(\"header\", True).load(\"cust1.csv\")\n",
    "order_df=spark.read.format(\"csv\").option(\"inferSchema\", True).option(\"header\", True).load(\"order1.csv\")\n",
    "\n",
    "join_df=cust_df.join(order_df, cust_df.id==order_df.customerId, \"inner\")\n",
    "\n",
    "cust_with_order_df=join_df.select(cust_df[\"id\"])\n",
    "\n",
    "filtered_customer_df= cust_df.filter(~cust_df[\"id\"].isin([row.id for row in cust_with_order_df.collect()]))\n",
    "\n",
    "result_df=filtered_customer_df.orderBy(\"name\")\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "\n",
    "'''\n",
    "Question 6:\n",
    "Convert below query into pyspark code:\n",
    "\n",
    "with result as (\n",
    "select distinct caller_id, count(caller_id)\n",
    "from calls c inner join users u\n",
    "on c.caller_id=u.user_id\n",
    "group by caller_id\n",
    "having count(caller_id)>=3)\n",
    "select count(*) from result;\n",
    "'''\n",
    "\n",
    "calls_df=spark.read.format(\"csv\").option(\"inferSchema\", True).option(\"header\", True).load(\"calls1.csv\")\n",
    "users_df=spark.read.format(\"csv\").option(\"inferSchema\", True).option(\"header\", True).load(\"users1.csv\")\n",
    "\n",
    "join_df=calls_df.join(users_df, calls_df.caller_id==users_df.user_id, \"inner\")\n",
    "\n",
    "group_df=join_df.groupBy(\"caller_id\").count()\n",
    "\n",
    "filtered_df=group_df.filter(col(\"count\")>=3)\n",
    "\n",
    "filtered_df.count()\n",
    "\n",
    "\n",
    "'''\n",
    "Question 7:\n",
    "Convert below query into pyspark code:\n",
    "\n",
    "select rentalHistory.room_id, \n",
    "count(rentalHistory.room_id) as 'no_of_bookings',\n",
    "sum(rentalHistory.number_nights * rooms.price) as 'total_earnings' from rentalHistory\n",
    "inner join rooms on rooms.room_id=rentalHistory.room_id\n",
    "group by rentalHistory.room_id\n",
    "order by no_of_bookings desc ;\n",
    "'''\n",
    "\n",
    "rooms_df=spark.read.format(\"csv\").option(\"header\", True).load(\"rooms.csv\")\n",
    "hist_df=spark.read.format(\"csv\").option(\"header\", True).load(\"hist.csv\")\n",
    "\n",
    "join_df=hist_df.join(rooms_df, hist_df.room_id==rooms_df.room_id, \"inner\")\n",
    "\n",
    "result_df=join_df.groupBy(hist_df.room_id).agg(count(hist_df.room_id).alias(\"no_of_bookings\"), \n",
    "                                         sum(hist_df.number_nights*rooms_df.price).alias(\"total_earnings\")).orderBy(desc(\"no_of_bookings\"))\n",
    "result_df.show()\n",
    "\n",
    "\n",
    "'''\n",
    "Question 8:\n",
    "convert below query into pyspark code:\n",
    "\n",
    "select d.restaurant_name, sum(o.sales_amount) as sales from delivery_orders d\n",
    "inner join order_value o\n",
    "on \n",
    "d.delivery_id=o.delivery_id\n",
    "and d.actual_delivery_time is not null\n",
    "group by  d.restaurant_name\n",
    "order by sales desc\n",
    "limit 2;\n",
    "'''\n",
    "\n",
    "delivery_df=spark.read.format(\"csv\").option(\"header\", True).load(\"delivery.csv\")\n",
    "order_df=spark.read.format(\"csv\").option(\"header\", True).load(\"order.csv\")\n",
    "\n",
    "result_df=delivery_df.join(order_df, delivery_df.delivery_id==order_df.delivery_id, \"inner\")\\\n",
    ".filter(order_df.actual_delivery_time.isNotNull())\\\n",
    ".groupBy(order_df.restaurant_name)\\\n",
    ".agg(sum(delivery_df.sales_amount).alias('sales'))\\\n",
    ".orderBy(desc('sales')).limit(2)\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "'''\n",
    "Question 9:\n",
    "convert below query into pyspark code:\n",
    "select driver_id, round(avg(order_total),3) as avg_total\n",
    "from delivery_details\n",
    "where datediff(minute,delivered_to_consumer_datetime,driver_at_restaurant_datetime )<=45\n",
    "group by driver_id\n",
    "having round(avg(order_total),3)>30\n",
    "order by avg_total desc;\n",
    "'''\n",
    "\n",
    "result_df=df1.withColumn(\"time_diff\", (unix_timestamp(\"delivered_to_consumer_datetime\")-unix_timestamp(\"driver_at_restaurant_datetime\"))/60)\\\n",
    ".filter(col(\"time_diff\")<=45)\\\n",
    ".groupBy(col(\"driver_id\"))\\\n",
    ".agg(round(avg(col(\"order_total\")),3))\\\n",
    ".alias(\"avg_total\")\\\n",
    ".filter(col(\"avg_total\")>30)\\\n",
    ".orderBy(desc(col(\"avg_total\")))\\\n",
    ".show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 10:\n",
    "convert below query into pyspark code:\n",
    "select shipment_id, weight\n",
    "from \n",
    "(select shipment_id, weight, dense_rank() over(order by weight desc) as ranking from shipments) as a\n",
    "where ranking=3\n",
    "\n",
    "'''\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"shipments.csv\")\n",
    "window=Window.orderBy(col(\"weight\").desc())\n",
    "df1=df.withColumn(\"ranking\", dense_rank().over(window))\n",
    "df2=df1.filter(df1.ranking==3)\n",
    "df1.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 11:\n",
    "convert below query to pyspark code:\n",
    "\n",
    "with cte as (\n",
    "select *, rank() over (partition by company order by year) as rank_by_year,\n",
    "rank() over (partition by company order by revenue) as rank_by_revenue,\n",
    "(cast(rank() over (partition by company order by year) as signed)\n",
    " - cast(rank() over (partition by company order by revenue)as signed)) as diff\n",
    "from company_revenue\n",
    ")\n",
    "\n",
    "select company from cte\n",
    "group by company\n",
    "having count(distinct diff)=1 and avg(diff)=0;\n",
    "'''\n",
    "\n",
    "company_df=spark.read.format('csv').option('header', True).load(\"company_revenue.csv\")\n",
    "\n",
    "window=Window.partitionBy(col(\"company\")).orderBy(col(\"year\"))\n",
    "window2=Window.partitionBy(col(\"company\")).orderBy(col(\"revenue\"))\n",
    "\n",
    "result_df=company_df.withColumn(\"rank_by_year\", rank().over(window))\\\n",
    ".withColumn(\"rank_by_revenue\", rank().over(window2))\\\n",
    ".withColumn(\"diff\", rank().over(window)- rank().over(window2))\\\n",
    "\n",
    "final_df=result_df.groupBy(col(\"company\"))\\\n",
    ".agg(countDistinct(\"diff\").alias(\"distinct_diff_count\"), avg(\"diff\").alias(\"avg_diff\"))\\\n",
    ".filter((col(\"distinct_diff_count\")==1) & (col(\"avg_diff\")==0))\n",
    "\n",
    "final_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 12:\n",
    "convert below query into pyspark code:\n",
    "\n",
    "with cte as (\n",
    "select *,\n",
    "row_number() over (partition by subj_1, subj_2 order by marks) as ranks\n",
    "from my_table1\n",
    ")\n",
    "\n",
    "select id, subj_1, subj_2, marks from cte where ranks<=1;\n",
    "\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"my_table1.csv\")\n",
    "window=Window.partitionBy(\"subj_1\", \"subj_2\").orderBy(\"marks\")\n",
    "\n",
    "result_df=df.withColumn(\"ranks\", row_number().over(window))\n",
    "\n",
    "final_df=result_df.filter(col(\"ranks\")<=1)\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "final_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 13:\n",
    "convert below query into pyspark code:\n",
    "\n",
    " with cte as(\n",
    " select sname, marks, row_number() over (partition by sname order by marks desc) as result\n",
    " from students1\n",
    ")\n",
    "\n",
    "select sname, sum(marks) as total from cte\n",
    "where result<=2\n",
    "group by sname\n",
    "\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"students_1.csv\")\n",
    "\n",
    "window=Window.partitionBy(\"sname\").orderBy(desc(\"marks\"))\n",
    "\n",
    "result_df=df.withColumn(\"results\", row_number().over(window))\n",
    "\n",
    "final_df=result_df.filter(result_df.results<=2)\\\n",
    ".groupBy(\"sname\")\\\n",
    ".agg(sum(result_df.marks).alias(\"total\"))\n",
    "\n",
    "final_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 14:\n",
    "convert below query into pyspark code:\n",
    "\n",
    "with cte as(\n",
    "select *, row_number() over(partition by subjects order by marks) as result\n",
    "from student_marks\n",
    ")\n",
    "select student_id, student_name, subjects, marks from cte\n",
    "where result<=2;\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"student_marks.csv\")\n",
    "\n",
    "#df.show()\n",
    "window=Window.partitionBy(\"subjects\").orderBy(\"marks\")\n",
    "\n",
    "result_df=df.withColumn(\"result\", row_number().over(window))\n",
    "\n",
    "#result_df.show()\n",
    "\n",
    "final_df=result_df.filter(col(\"result\")<=2).select(\"student_id\",\"student_name\",\"subjects\",\"marks\")\n",
    "\n",
    "final_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 15:\n",
    "Calculate average marks, count of students for each subject\n",
    "\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"student_marks.csv\")\n",
    "\n",
    "result_df=df.groupBy(\"subjects\")\\\n",
    ".agg(avg(\"marks\").alias(\"avg_marks\"), count(\"student_name\").alias(\"count\"))\\\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 16:\n",
    "Calculate Running total\n",
    "\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"student_marks.csv\")\n",
    "\n",
    "window=Window.partitionBy(\"subjects\").orderBy(\"student_id\")\n",
    "\n",
    "result_df=df.withColumn(\"result\", sum(col(\"marks\")).over(window))\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 17:\n",
    "To calculate the percentage of total salary that each employee contributes to their respective department.\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"student_marks.csv\")\n",
    "\n",
    "#df.show()\n",
    "\n",
    "df1=df.groupBy(\"subjects\").agg(sum(\"marks\").alias(\"total_marks\"))\n",
    "\n",
    "#df1.show()\n",
    "\n",
    "join_df=df.join(df1, df.subjects==df1.subjects)\n",
    "\n",
    "join_df.show()\n",
    "\n",
    "result_df=join_df.withColumn(\"percentage\", round((col(\"marks\")/col(\"total_marks\"))*100, 2))\n",
    "\n",
    "result_df.show()\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Question 18:\n",
    "Calculate Running average\n",
    "\n",
    "'''\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\", True).load(\"student_marks.csv\")\n",
    "\n",
    "window=Window.partitionBy(\"subjects\").orderBy(\"student_id\")\n",
    "\n",
    "result_df=df.withColumn(\"average\", avg(col(\"marks\")).over(window))\n",
    "\n",
    "result_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
