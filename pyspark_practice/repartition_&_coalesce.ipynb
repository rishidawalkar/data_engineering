{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7aeef47e-6904-4fd1-a81c-a2f91f58b0c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###The Difference between REPARTITION() and COALESCE() is:\n",
    "###REPARTITION() EVENLY DISTRIBUTES THE DATA WITHIN THE PARTITIONS , WHEREAS COALESCE() UNEVENLY/RANDOMLY DISTRIBUTES THE DATA \n",
    "###REPARTITION() IS USED TO INCREASE OR DECREASE THE NO. OF PARTITIONS, WHEREAS COALESCE() IS ONNLY USED TO REDUCE() THE NO. OF PARTITIONS.\n",
    "\n",
    "import pyspark, os, sys\n",
    "from pyspark.sql import *\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import types\n",
    "\n",
    "spark=SparkSession.builder.getOrCreate()\n",
    "\n",
    "#To check how many default partitions will get created   --the result should be 8 partitions\n",
    "#spark.defaultParallelism\n",
    "\n",
    "#To check how much max amount of data can be processed on each partition   --the result should be 128 MB\n",
    "#spark.conf.get(\"spark.sql.files.maxPartitonBytes\")\n",
    "\n",
    "#To change the no. of partitions \n",
    "#spark.conf.get(\"spark.sql.files.maxPartitionBytes\",size of each partition)\n",
    "\n",
    "spark.conf.set(\"spark.sql.files.maxPartitionBytes\", 200000) #here we are setting size as 200KB per partition \n",
    "spark.conf.get(\"spark.sql.files.maxPartitionBytes\")\n",
    "\n",
    "\n",
    "#To check no. of partitions we use getNumPartitions()\n",
    "#rdd=spark.sparkContext.textFile(r'C:\\Users\\HP\\Downloads\\sample.csv')\n",
    "#rdd.getNumPartitions()\n",
    "\n",
    "df=spark.read.format(\"csv\").option(\"header\",True).option(\"sep\",\",\").option(\"inferSchema\",True).load(\"googleplaystore.csv\")\n",
    "df.rdd.getNumPartitions() # we have got result as 7 partition, as file size is 1.3 MB i.e, 1300000, so 1300000/200000 is approx 7\n",
    "\n",
    "#to adjust the no. of partitions, we will use repartition(no. of partitions)\n",
    "df1=df.repartition(20)\n",
    "df1.rdd.getNumPartitions()\n",
    "\n",
    "#to reduce the mo. of partitions, we can use coalesce()\n",
    "\n",
    "df2=df1.coalesce(2)\n",
    "df2.rdd.getNumPartitions()  #from 20 partitions, it will get reduced to 2 partitions\n",
    "\n",
    "#to check how data is distributed within each partition\n",
    "#df.rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891f91bd-0236-4ed4-806d-2f9afc86ca27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
