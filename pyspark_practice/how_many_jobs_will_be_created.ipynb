{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "796aca17-4cfd-4530-aa0a-fd37b4b5735b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------+\n",
      "|Store_Id|      Item|Amount|\n",
      "+--------+----------+------+\n",
      "|    NULL|      Item|  NULL|\n",
      "|       1|  Cosmetic|   150|\n",
      "|       2|Stationary|   250|\n",
      "|       3|   Apparel|   180|\n",
      "|       4|      Food|   500|\n",
      "|       5|   Leisure|   300|\n",
      "|       1|       Deo|   100|\n",
      "|       2|     Books|   200|\n",
      "|       3|   Tshirts|   120|\n",
      "|       4|   Burgers|   200|\n",
      "|       5|     Movie|   180|\n",
      "+--------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark, os, sys\n",
    "from pyspark.sql import *\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import types\n",
    "from pyspark import StorageLevel\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "spark=SparkSession.builder.getOrCreate()\n",
    "\n",
    "#For below scenario, only 1 Job will be created. which will be used to go through one entire record of csv file to calculate its no. of columns\n",
    "df=spark.read.format(\"csv\").load(\"products.csv\")\n",
    "#df.show()\n",
    "#df.explain(True)\n",
    "\n",
    "#For below example, 2 Jobs are created. First will be to go through one entire record of csv file to calculate no. of columns and 2nd job will again go though entire one record to calculate datatypes of the columns.\n",
    "df_2=spark.read.format(\"csv\").option(\"inferSchema\", True).load(\"products.csv\")\n",
    "#df_2.explain(True)\n",
    "#df_2.show()\n",
    "\n",
    "#For below example, No jobs will be created as we are defining the schema here itself with column names and its datatypes as below, hence there is no need to scan the data\n",
    "schema=StructType([\n",
    "    StructField(\"Store_Id\", IntegerType(), True),\n",
    "    StructField(\"Item\", StringType(), True),\n",
    "    StructField(\"Amount\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df_3=spark.read.format(\"csv\").schema(schema).load(\"products.csv\")\n",
    "df_3.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
